{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AQpuhSkBTKjE"
   },
   "source": [
    "## Аппроксимация Q-функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3KgKLUiJTKjI"
   },
   "source": [
    "В этой тетрадке мы будем использовать библиотеку tensorflow для обучения нейронной сети, хотя можно использовать и любую другую библиотеку. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SDsUrk4tTKjK"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7IMFc1dTTKjP"
   },
   "source": [
    "Будем тестировать наши модели на классической задаче с перевернутым маятником:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8pb2lfxTKjR",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f75434bfba8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vX54sySbTKjV"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bk3-9NN5TKjZ"
   },
   "source": [
    "# Глубокое Q-обучение: построение сети\n",
    "\n",
    "Так как описание состояния в задаче с маятником представляет собой не \"сырые\" признаки, а уже предобработанные (координаты, углы), нам не нужна для начала сложная архитектура, начнем с такой:\n",
    "\n",
    "<img src=\"qlearningscheme.png\" caption=\"Архитектура сети\">\n",
    "Для начала попробуйте использовать только полносвязные слои (__L.Dense__) и линейные активационные функции. Сигмоиды и другие функции не будут работать с ненормализованными входными данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KHi0meFMTKja",
    "outputId": "3d9db427-b07f-4b40-a9ee-9b643410def4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers as L\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M6ZJDOPJTKjd"
   },
   "outputs": [],
   "source": [
    "network = keras.models.Sequential()\n",
    "network.add(L.InputLayer(state_dim))\n",
    "# строим сеть!\n",
    "#~~~~~~~~~~ Решение ~~~~~~~~~~~~~~~\n",
    "\n",
    " \n",
    "network.add(L.Dense(300, activation=\"relu\"))\n",
    "network.add(L.Dense(n_actions))\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kkOAJbTHTKji"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def get_action(state, epsilon=0):\n",
    "    \"\"\"\n",
    "    сэмплируем (eps greedy) действие  \n",
    "    \"\"\"\n",
    "    q_values = network.predict(state[None])[0]\n",
    "    \n",
    "    ### Ваш код здесь - нужно выбрать действия eps-жадно\n",
    "    # action = \n",
    "    #~~~~~~~~~~ Решение ~~~~~~~~~~~~~~~\n",
    "    \n",
    "     \n",
    "    if epsilon < random.random():\n",
    "        action = np.argmax(q_values)\n",
    "    else:\n",
    "        action = random.choice(range(n_actions))\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "zsTrcYTSTKjk",
    "outputId": "86e4215c-366f-48fd-ee6f-e094053aa4b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1002 11:30:24.976516 140142519334720 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=0.0 tests passed\n",
      "e=0.1 tests passed\n",
      "e=0.5 tests passed\n",
      "e=1.0 tests passed\n"
     ]
    }
   ],
   "source": [
    "assert network.output_shape == (None, n_actions), \\\n",
    "    \"убедитесь, что стратегия переводит \\\n",
    "    s -> [Q(s,a0), ..., Q(s, a_last)]\"\n",
    "assert network.layers[-1].activation == \\\n",
    "       keras.activations.linear, \\\n",
    "    \"убедитесь, что вы предсказываете q без нелинейности\"\n",
    "# проверяем исследование\n",
    "s = env.reset()\n",
    "assert np.shape(get_action(s)) == (), \\\n",
    "    \"убедитесь, что возвращаете одно действие\"\n",
    "for eps in [0., 0.1, 0.5, 1.0]:\n",
    "    na = n_actions\n",
    "    st = np.bincount([get_action(s, epsilon=eps) \\\n",
    "                      for i in range(10000)],\n",
    "                     minlength=na)\n",
    "    ba = st.argmax()\n",
    "    assert abs(\n",
    "        st[ba] - 10000 * (1 - eps + eps / na)) < 200\n",
    "    for oa in range(na):\n",
    "        if oa != ba:\n",
    "            assert abs(st[oa] - 10000 * (eps / na)) < 200\n",
    "    print('e=%.1f tests passed' % eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YyKpjfO7TKjq"
   },
   "source": [
    "### Q-обучение через градиентный спуск\n",
    "\n",
    "Теперь будем приближать Q-функцию агента, минимизируя TD функцию потерь:\n",
    "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2. $$\n",
    "\n",
    "Основная тонкость состоит в использовании  $Q_{-}(s',a')$. Эта та же самая функция, что и $Q_{\\theta}$, которая является выходом нейронной сети, но при обучении сети, мы не пропускаем через эти слои градиенты. Для этого используется функция `tf.stop_gradient`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nnd2924Md24U"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOw0pH5WTKjs"
   },
   "outputs": [],
   "source": [
    "# Создаем placeholders для <s, a, r, s'>, \n",
    "# а также индикатор окончания эпизода (is_done = True)\n",
    "states_ph = tf.placeholder('float32', \n",
    "                           shape=(None,) + state_dim)\n",
    "actions_ph = tf.placeholder('int32', shape=[None])\n",
    "rewards_ph = tf.placeholder('float32', shape=[None])\n",
    "next_states_ph = tf.placeholder('float32', \n",
    "                           shape=(None,) + state_dim)\n",
    "is_done_ph = tf.placeholder('bool', shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L6MRQ2z6TKjx"
   },
   "outputs": [],
   "source": [
    "# получаем q для всех действий, в текущем состоянии\n",
    "predicted_qvalues = network(states_ph)\n",
    "\n",
    "# получаем q-values для выбранного действия\n",
    "predicted_qvalues_for_actions =\\\n",
    "tf.reduce_sum(\n",
    "predicted_qvalues * tf.one_hot(actions_ph, n_actions),\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T0Sb5q9uTKjz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1002 11:30:51.522647 140142519334720 deprecation.py:323] From <ipython-input-10-a831ceba3ea3>:35: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "# применяем сеть для получения q-value для next_states_ph\n",
    "# predicted_next_qvalues =\n",
    "#~~~~~~~~~~ Решение ~~~~~~~~~~~~~~~\n",
    "\n",
    " \n",
    "predicted_next_qvalues = network(next_states_ph)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# вычисляем V*(next_states) \n",
    "# по предсказанным следующим q-values\n",
    "# next_state_values =\n",
    "#~~~~~~~~~~ Решение ~~~~~~~~~~~~~~~\n",
    "\n",
    " \n",
    "next_state_values = tf.reduce_max(predicted_next_qvalues, axis=1)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# Вычисляем target q-values для функции потерь \n",
    "# target_qvalues_for_actions = \n",
    "#~~~~~~~~~~ Решение ~~~~~~~~~~~~~~~\n",
    "\n",
    " \n",
    "target_qvalues_for_actions = rewards_ph + gamma * next_state_values\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# для последнего действия используем \n",
    "# упрощенную формулу Q(s,a) = r(s,a) \n",
    "target_qvalues_for_actions =\\\n",
    "tf.where(is_done_ph, rewards_ph, \n",
    "         target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aA15Fi_8d4hL"
   },
   "source": [
    "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2. $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LK9euKgoTKj2"
   },
   "outputs": [],
   "source": [
    "### среднеквадратичная функция потерь stop_gradient\n",
    "# loss = \n",
    "#~~~~~~~~~~ Решение ~~~~~~~~~~~~~~~\n",
    "\n",
    " \n",
    "loss = (predicted_qvalues_for_actions - tf.stop_gradient(target_qvalues_for_actions))**2\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# применяем AdamOptimizer\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5oZQ957tTKj5"
   },
   "outputs": [],
   "source": [
    "assert tf.gradients(loss, \\\n",
    "    [predicted_qvalues_for_actions])[0] is not None, \\\n",
    "\"убедитесь, что обновление выполняется\\\n",
    "только для выбранного действия\"\n",
    "assert tf.gradients(loss, \\\n",
    "    [predicted_next_qvalues])[0] is None, \\\n",
    "\"убедитесь, что вы не распространяете градиент Q_(s',a')\"\n",
    "assert predicted_next_qvalues.shape.ndims == 2, \\\n",
    "\"убедитесь, что вы предсказываете q для всех действий,\\\n",
    "следующего состояния\"\n",
    "assert next_state_values.shape.ndims == 1, \\\n",
    "\"убедитесь, что вы вычислили V(s') как максимум\\\n",
    "только по оси действий, а не по всем осям\"\n",
    "assert target_qvalues_for_actions.shape.ndims == 1, \\\n",
    "\"что-то не так с целевыми q-значениями,\\\n",
    "они должны быть вектором\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "adITE6BaTKj-"
   },
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YWIDEK7ATKkA"
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000, epsilon=0, train=False):\n",
    "    \"\"\"генерация сессии\"\"\"\n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        a = get_action(s, epsilon=epsilon)       \n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        \n",
    "        if train:\n",
    "            sess.run(train_step,{\n",
    "                states_ph: [s], actions_ph: [a], \n",
    "                rewards_ph: [r], next_states_ph: [next_s], \n",
    "                is_done_ph: [done]\n",
    "            })\n",
    "\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TCeeMefwTKkC"
   },
   "outputs": [],
   "source": [
    "epsilon = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "-fMjfHxiTKkE",
    "outputId": "cc8bb4c7-37ff-49f8-bb38-ed132adbc730",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Error while reading resource variable beta1_power from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/beta1_power/N10tensorflow3VarE does not exist.\n\t [[node Adam/update_dense_1/kernel/ResourceApplyAdam/ReadVariableOp (defined at <ipython-input-11-e61309f3b57c>:12) ]]\n\nOriginal stack trace for 'Adam/update_dense_1/kernel/ResourceApplyAdam/ReadVariableOp':\n  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2854, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2880, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3057, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3248, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3325, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-e61309f3b57c>\", line 12, in <module>\n    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/optimizer.py\", line 413, in minimize\n    name=name)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/optimizer.py\", line 614, in apply_gradients\n    update_ops.append(processor.update_op(self, grad))\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/optimizer.py\", line 171, in update_op\n    update_op = optimizer._resource_apply_dense(g, self._v)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/adam.py\", line 177, in _resource_apply_dense\n    use_locking=self._use_locking)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/gen_training_ops.py\", line 1399, in resource_apply_adam\n    use_nesterov=use_nesterov, name=name)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 527, in _apply_op_helper\n    preferred_dtype=default_dtype)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 1224, in internal_convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1507, in _dense_var_to_tensor\n    return var._dense_var_to_tensor(dtype=dtype, name=name, as_ref=as_ref)  # pylint: disable=protected-access\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1462, in _dense_var_to_tensor\n    return self.value()\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 851, in value\n    return self._read_variable_op()\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 935, in _read_variable_op\n    self._dtype)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 587, in read_variable_op\n    \"ReadVariableOp\", resource=resource, dtype=dtype, name=name)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Error while reading resource variable beta1_power from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/beta1_power/N10tensorflow3VarE does not exist.\n\t [[{{node Adam/update_dense_1/kernel/ResourceApplyAdam/ReadVariableOp}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-039bac1e1d94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     session_rewards = [generate_session(epsilon=epsilon, \n\u001b[0;32m----> 3\u001b[0;31m                         train=True) for _ in range(500)]\n\u001b[0m\u001b[1;32m      4\u001b[0m     print(\"epoch #{}\\tmean r = {:.3f}\\tepsilon = {:.3f}\"\n\u001b[1;32m      5\u001b[0m           .format(i, np.mean(session_rewards), epsilon))\n",
      "\u001b[0;32m<ipython-input-15-039bac1e1d94>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     session_rewards = [generate_session(epsilon=epsilon, \n\u001b[0;32m----> 3\u001b[0;31m                         train=True) for _ in range(500)]\n\u001b[0m\u001b[1;32m      4\u001b[0m     print(\"epoch #{}\\tmean r = {:.3f}\\tepsilon = {:.3f}\"\n\u001b[1;32m      5\u001b[0m           .format(i, np.mean(session_rewards), epsilon))\n",
      "\u001b[0;32m<ipython-input-13-a7cbb0fd214a>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(t_max, epsilon, train)\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mstates_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mrewards_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnext_s\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0mis_done_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             })\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Error while reading resource variable beta1_power from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/beta1_power/N10tensorflow3VarE does not exist.\n\t [[node Adam/update_dense_1/kernel/ResourceApplyAdam/ReadVariableOp (defined at <ipython-input-11-e61309f3b57c>:12) ]]\n\nOriginal stack trace for 'Adam/update_dense_1/kernel/ResourceApplyAdam/ReadVariableOp':\n  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2854, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2880, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3057, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3248, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3325, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-e61309f3b57c>\", line 12, in <module>\n    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/optimizer.py\", line 413, in minimize\n    name=name)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/optimizer.py\", line 614, in apply_gradients\n    update_ops.append(processor.update_op(self, grad))\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/optimizer.py\", line 171, in update_op\n    update_op = optimizer._resource_apply_dense(g, self._v)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/adam.py\", line 177, in _resource_apply_dense\n    use_locking=self._use_locking)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/gen_training_ops.py\", line 1399, in resource_apply_adam\n    use_nesterov=use_nesterov, name=name)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 527, in _apply_op_helper\n    preferred_dtype=default_dtype)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 1224, in internal_convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1507, in _dense_var_to_tensor\n    return var._dense_var_to_tensor(dtype=dtype, name=name, as_ref=as_ref)  # pylint: disable=protected-access\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1462, in _dense_var_to_tensor\n    return self.value()\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 851, in value\n    return self._read_variable_op()\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 935, in _read_variable_op\n    self._dtype)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 587, in read_variable_op\n    \"ReadVariableOp\", resource=resource, dtype=dtype, name=name)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    session_rewards = [generate_session(epsilon=epsilon, \n",
    "                        train=True) for _ in range(500)]\n",
    "    print(\"epoch #{}\\tmean r = {:.3f}\\tepsilon = {:.3f}\"\n",
    "          .format(i, np.mean(session_rewards), epsilon))\n",
    "    \n",
    "    epsilon *= 0.95\n",
    "    epsilon = max(0.1, epsilon)\n",
    "    assert epsilon >= 1e-4, \\\n",
    "    \"убедитесь, что epsilon не становится < 0\"\n",
    "    if np.mean(session_rewards) > 300:\n",
    "        print (\"Принято!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FSC00yvzTKkH"
   },
   "source": [
    "### Интерпретация результатов\n",
    "\n",
    "\\begin{itemize}\n",
    "\\item mean reward -- среднее вознаграждение за эпизод. В случае корректной реализации, этот показатель будет низким первые 5 эпох и только затем будет возрастать и сойдется на 20-30 эпох в зависииости от архитектуры сети.\n",
    "\\item Если сеть не достигает нужных результатов к концу цикла, попробуйте увеличить число нейронов в скрытом слое или поменяйте $\\epsilon$.\n",
    "\\item epsilon -- обеспечивает стремление агента исследовать среду. Можно искусственно изменять малые значения $\\epsilon$ при низких результатах на 0.1 - 0.5.\n",
    "\\end{itemize}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sem4_approx.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
