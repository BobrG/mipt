{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CpceefzmsN45"
   },
   "source": [
    "## Динамическое программирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uX-Ky982sN47"
   },
   "source": [
    "Рассмотрим алгоритм итерации по оценкам состояния $V$ (Value Iteration):\n",
    "    $$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n",
    "На основе оценки $V_i$ можно посчитать функцию оценки $Q_i$ действия $a$ в состоянии $s$:\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n",
    "$$V_{(i+1)}(s) = \\max_a Q_i(s,a)$$\n",
    "\n",
    "Зададим напрямую модель MDP с картинки:\n",
    "<img src=\"mdp.png\" caption=\"Марковский процесс принятия решений\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ebF4HaPsN48"
   },
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    "  's0':{\n",
    "    'a0': {'s0': 0.5, 's2': 0.5},\n",
    "    'a1': {'s2': 1}\n",
    "  },\n",
    "  's1':{\n",
    "    'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "    'a1': {'s1': 0.95, 's2': 0.05}\n",
    "  },\n",
    "  's2':{\n",
    "    'a0': {'s0': 0.4, 's1': 0.6},\n",
    "    'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "  }\n",
    "}\n",
    "rewards = {\n",
    "  's1': {'a0': {'s0': +5}},\n",
    "  's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "import numpy as np\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "0bcgBOZbsN5A",
    "outputId": "9ff86175-e1cd-4d8b-d5ec-72f1cfb23cd4"
   },
   "outputs": [],
   "source": [
    "print(\"all_states =\", mdp.get_all_states())\n",
    "print(\"possible_actions('s1') = \", \n",
    "      mdp.get_possible_actions('s1'))\n",
    "print(\"next_states('s1', 'a0') = \", \n",
    "      mdp.get_next_states('s1', 'a0'))\n",
    "print(\"reward('s1', 'a0', 's0') = \", \n",
    "      mdp.get_reward('s1', 'a0', 's0'))\n",
    "print(\"transition_prob('s1', 'a0', 's0') = \", \n",
    "      mdp.get_transition_prob('s1', 'a0', 's0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WYDjsDv2sN5F"
   },
   "source": [
    "### Задание 1\n",
    "\n",
    "Реализуем итерационное вычисление функций $V$ и $Q$ и применим их для заданного вручную MDP. Вначале вычисляем оценку состояния-действия:\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zVSz8FCRsN5G"
   },
   "outputs": [],
   "source": [
    "def get_action_value(mdp, state_values, state, action, \n",
    "                     gamma):\n",
    "    \"\"\" Вычислеям Q(s,a) по формуле выше \"\"\"\n",
    "    # вычислеяем оценку состояния\n",
    "    # Q = \n",
    "    #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~    \n",
    "    raise NotImplementedError    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U8rRYvrpsN5I"
   },
   "outputs": [],
   "source": [
    "test_Vs = {s : i for i, s in \n",
    "    enumerate(sorted(mdp.get_all_states()))}\n",
    "assert np.allclose(\n",
    "    get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1tEz93sasN5K"
   },
   "source": [
    "Теперь оцениваем полезность самого состояния:\n",
    "\n",
    "$$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QA8c-2_fsN5K"
   },
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\" Считаем следующее V(s) по формуле выше.\"\"\"\n",
    "    if mdp.is_terminal(state): \n",
    "        return 0\n",
    "    # V = \n",
    "    #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~    \n",
    "    raise NotImplementedError    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ym4og12hsN5N"
   },
   "outputs": [],
   "source": [
    "test_Vs_copy = dict(test_Vs)\n",
    "assert np.allclose(\n",
    "    get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "emA2sv-QsN5P"
   },
   "source": [
    "Теперь создаем основной цикл итерационного оценки полезности состояний с критерием остановки, который проверяет насколько изменились оценки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "er9KwtjBsN5Q"
   },
   "outputs": [],
   "source": [
    "def value_iteration(mdp, state_values=None,\n",
    "    gamma = 0.9, num_iter = 1000, min_difference = 1e-5):\n",
    "    \"\"\" выполняет num_iter шагов итерации по значениям\"\"\"\n",
    "    # инициализируем V(s)\n",
    "    state_values = state_values or \\\n",
    "    {s : 0 for s in mdp.get_all_states()}\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        # Вычисляем новые полезности состояний, \n",
    "        # используя функции, определенные выше. \n",
    "        # Должен получиться словарь {s: new_V(s)}\n",
    "        # new_state_values = \n",
    "        #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~        \n",
    "        raise NotImplementedError        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "        assert isinstance(new_state_values, dict)\n",
    "\n",
    "        # Считаем разницу\n",
    "        # diff = \n",
    "        #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~        \n",
    "        raise NotImplementedError        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "        print(\"iter %4i | diff: %6.5f | V(start): %.3f \"%\n",
    "          (i, diff, new_state_values[mdp._initial_state]))\n",
    "        \n",
    "        state_values = new_state_values\n",
    "        if diff < min_difference:\n",
    "            print(\"Принято! Алгоритм сходится!\")\n",
    "            break\n",
    "            \n",
    "    return state_values\n",
    "\n",
    "state_values = value_iteration(mdp,\n",
    "        num_iter = 100, min_difference = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3cKoN2-tsN5V"
   },
   "outputs": [],
   "source": [
    "print(\"Final state values:\", state_values)\n",
    "assert abs(state_values['s0'] - 8.032)  < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bSIBT-49sN5X"
   },
   "source": [
    "По найденным полезностям и зная модель переходов легко найти оптимальную стратегию:\n",
    "$$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x9m7NArGsN5Y"
   },
   "outputs": [],
   "source": [
    "def get_optimal_action(mdp, state_values, state, \n",
    "                       gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state): return None\n",
    "    \n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    # выбираем лучшее действие\n",
    "    # i = \n",
    "    #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~    \n",
    "    raise NotImplementedError    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    return actions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tCt9IjLnsN5Z"
   },
   "outputs": [],
   "source": [
    "assert get_optimal_action(mdp, \n",
    "        state_values, 's0', gamma=0.9) == 'a1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-omPxnyksN5b"
   },
   "source": [
    "### Задание 2\n",
    "\n",
    "Теперь проверим работу итерации по ценностям на классической задаче FrozenLake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QhBAOCyEsN5c"
   },
   "outputs": [],
   "source": [
    "from mdp import FrozenLakeEnv\n",
    "mdp = FrozenLakeEnv(slip_chance=0)\n",
    "\n",
    "mdp.render()\n",
    "state_values = value_iteration(mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-kS3QDqNsN5h"
   },
   "source": [
    "Визуализируем нашу стратегию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wRbL3izdsN5h"
   },
   "outputs": [],
   "source": [
    "def draw_policy(mdp, state_values, gamma=0.9):\n",
    "    \"\"\"функция визуализации стратегии\"\"\"\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    h, w = mdp.desc.shape\n",
    "    states = sorted(mdp.get_all_states())\n",
    "    V = np.array([state_values[s] for s in states])\n",
    "    Pi = {\n",
    "        s: get_optimal_action(mdp, state_values, s, gamma)\n",
    "        for s in states}\n",
    "    plt.imshow(V.reshape(w, h),\n",
    "               cmap='gray', interpolation='none',\n",
    "               clim=(0, 1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(h) - .5)\n",
    "    ax.set_yticks(np.arange(w) - .5)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    Y, X = np.mgrid[0:4, 0:4]\n",
    "    a2uv = {'left': (-1, 0), 'down': (0, -1),\n",
    "            'right': (1, 0), 'up': (-1, 0)}\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            plt.text(x, y, str(mdp.desc[y, x].item()),\n",
    "                     color='g', size=12,\n",
    "                     verticalalignment='center',\n",
    "                     horizontalalignment='center',\n",
    "                     fontweight='bold')\n",
    "            a = Pi[y, x]\n",
    "            if a is None: continue\n",
    "            u, v = a2uv[a]\n",
    "            plt.arrow(x, y, u * .3, -v * .3,\n",
    "                      color='m', head_width=0.1,\n",
    "                      head_length=0.1)\n",
    "    plt.grid(color='b', lw=2, ls='-')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQiqx1QesN5k"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "mdp = FrozenLakeEnv(map_name='8x8',slip_chance=0.1)\n",
    "state_values = {s : 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(30):\n",
    "    clear_output(True)\n",
    "    print(\"after iteration %i\"%i)\n",
    "    state_values = value_iteration(mdp, \n",
    "                            state_values, num_iter=1)\n",
    "    draw_policy(mdp, state_values)\n",
    "    sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62xktmhWsN5n"
   },
   "source": [
    "Тестируем на различных вариантах окружения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZsHbVNzsN5o"
   },
   "outputs": [],
   "source": [
    "# Получаем среднее вознаграждение агента\n",
    "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        # выполняем оптимальное действие в окружении\n",
    "        # s, r, done, _ = \n",
    "        #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~        \n",
    "        raise NotImplementedError        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "        rewards.append(r)\n",
    "        if done: break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "    \n",
    "print(\"Cреднее вознаграждение:\", np.mean(total_rewards))\n",
    "assert(0.6 <= np.mean(total_rewards) <= 0.8)\n",
    "print(\"Принято!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xdMVQ1XQsN5s"
   },
   "source": [
    "### Задание 3\n",
    "\n",
    "Теперь рассмотрим следующий алгоритм - итерации по стратегиям (PI):\n",
    "\\begin{enumerate}\n",
    "    \\item Инициализация $\\pi_0$   (например случайные действия)\n",
    "    \\item For $n=0, 1, 2, \\dots$\n",
    "    \\item Вычисление state-value функции $V^{\\pi_{n}}$\n",
    "    \\item Используя $V^{\\pi_{n}}$, рассчитываем функцию $Q^{\\pi_{n}}$\n",
    "    \\item Рассчитываем новую стратегию $\\pi_{n+1}(s) = \\operatorname*{argmax}_a Q^{\\pi_{n}}(s,a)$\n",
    "\\end{enumerate}\n",
    "\n",
    "PI включает в себя оценку полезности состояния как внутренний шаг."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Pg53dh5sN5s"
   },
   "source": [
    "Вначале оценим полезности, используя текущую стратегию:\n",
    "$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n",
    "    Мы будем искать точное решение, хотя могли использовать и предыдущий итерационный подход. Для этого будем решать систему линейных уравнений относительно $V^{\\pi}(s_i)$ с помощью np.linalg.solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sP2gL08osN5u"
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import solve\n",
    "\n",
    "def compute_vpi(mdp, policy, gamma):\n",
    "    \"\"\"\n",
    "    Считем V^pi(s) для всех состояний, согласно стратегии.\n",
    "    :param policy: словарь состояние->действие {s : a}\n",
    "    :returns: словарь {state : V^pi(state)}\n",
    "    \"\"\"\n",
    "    states = mdp.get_all_states()\n",
    "    A, b = [], []\n",
    "    for i, state in enumerate(states):\n",
    "        if state in policy:\n",
    "            a = policy[state]\n",
    "            # формируем матрицу A (... A.append(...))\n",
    "            #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~            \n",
    "            raise NotImplementedError            \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            \n",
    "            # и вектор b (b.append(...))\n",
    "            #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~            \n",
    "            raise NotImplementedError            \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            \n",
    "        else:\n",
    "            # формируем матрицу A (... A.append(...))\n",
    "            #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~            \n",
    "            raise NotImplementedError            \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            \n",
    "            # вектор b (b.append(...))\n",
    "            #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~            \n",
    "            raise NotImplementedError            \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            \n",
    "    A = np.array(A)\n",
    "    b = np.array(b)\n",
    "    \n",
    "    values = solve(A, b)\n",
    "    \n",
    "    state_values = {states[i] : values[i] \n",
    "                    for i in range(len(states))}\n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1aG9dcJVsN5w"
   },
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    "    's0': {\n",
    "        'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}\n",
    "    },\n",
    "    's1': {\n",
    "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}\n",
    "    },\n",
    "    's2': {\n",
    "        'a0': {'s0': 0.4, 's1': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "    }\n",
    "}\n",
    "rewards = {\n",
    "    's1': {'a0': {'s0': +5}},\n",
    "    's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')\n",
    "\n",
    "gamma = 0.9  # коэффициент дисконтирования для MDP\n",
    "\n",
    "test_policy = {\n",
    "    s: np.random.choice(mdp.get_possible_actions(s))\n",
    "    for s in mdp.get_all_states()}\n",
    "new_vpi = compute_vpi(mdp, test_policy, gamma)\n",
    "\n",
    "print(new_vpi)\n",
    "assert type(new_vpi) is dict, \\\n",
    "    \"функция compute_vpi должна возвращать словарь \\\n",
    "    {состояние s : V^pi(s) }\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hD-bBuwKsN5y"
   },
   "source": [
    "Теперь обновляем стратегию на основе новых значений полезностей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5pKbE1visN50"
   },
   "outputs": [],
   "source": [
    "def compute_new_policy(mdp, vpi, gamma):\n",
    "    \"\"\"\n",
    "    Рассчитываем новую стратегию\n",
    "    :param vpi: словарь {state : V^pi(state) }\n",
    "    :returns: словарь {state : оптимальное действие}\n",
    "    \"\"\"\n",
    "    Q = {}\n",
    "    for state in mdp.get_all_states():\n",
    "        Q[state] = {}\n",
    "        for a in mdp.get_possible_actions(state):\n",
    "            values = []\n",
    "            for next_state in mdp.get_next_states(state,\n",
    "                                                  a):\n",
    "                r = mdp.get_reward(state, a, next_state)\n",
    "                p = mdp.get_transition_prob(state, a,\n",
    "                                            next_state)\n",
    "                # values.append(...)\n",
    "                #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~                \n",
    "                raise NotImplementedError                \n",
    "                #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                \n",
    "            Q[state][a] = sum(values)\n",
    "\n",
    "    policy = {}\n",
    "    for state in mdp.get_all_states():\n",
    "        actions = mdp.get_possible_actions(state)\n",
    "        if actions:\n",
    "            # выбираем оптимальное действие в state\n",
    "            # policy[state] = ... \n",
    "            #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~            \n",
    "            raise NotImplementedError            \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SKkZnfGRsN53"
   },
   "outputs": [],
   "source": [
    "new_policy = compute_new_policy(mdp, new_vpi, gamma)\n",
    "\n",
    "print(new_policy)\n",
    "\n",
    "assert type(new_policy) is dict, \\\n",
    "\"функция compute_new_policy должна возвращать словарь \\\n",
    "{состояние s: оптимальное действие}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHax1qV5sN55"
   },
   "source": [
    "Собираем все в единый цикл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4rfeoFasN56"
   },
   "outputs": [],
   "source": [
    "def policy_iteration(mdp, policy=None, gamma = 0.9, \n",
    "                 num_iter = 1000, min_difference = 1e-5):\n",
    "    \"\"\" \n",
    "    Запускаем цикл итерации по стратегиям \n",
    "    Если стратегия не определена, задаем случайную\n",
    "    \"\"\"\n",
    "    for i in range(num_iter):\n",
    "        if not policy:\n",
    "            policy = {}\n",
    "            for s in mdp.get_all_states():\n",
    "                if mdp.get_possible_actions(s):\n",
    "                    policy[s] = np.random \\\n",
    "                    .choice(mdp.get_possible_actions(s))\n",
    "        # state_values = \n",
    "        #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~        \n",
    "        raise NotImplementedError        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "        # policy = \n",
    "        #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~        \n",
    "        raise NotImplementedError        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "    return state_values, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NJm34Gc5sN57"
   },
   "source": [
    "Тестируем на FrozenLake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NC7hXWhssN58"
   },
   "outputs": [],
   "source": [
    "mdp = FrozenLakeEnv(slip_chance=0.1)\n",
    "state_values, policy = policy_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(policy[s])\n",
    "        rewards.append(r)\n",
    "        if done: break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "    \n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.8 <= np.mean(total_rewards) <= 0.95)\n",
    "print(\"Принято!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sem2_dp.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
