{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic\n",
    "\n",
    "\n",
    "<img src=\"ac_scheme.png\">\n",
    "\n",
    "«Критик» оценивает функцию полезности. Это может быть полезность действия ($Q$) или полезность состояния ($V$).\n",
    "\n",
    "«Актор» обновляет распределение стратегии в направлении, предложенном Критиком.\n",
    "\n",
    "\n",
    "Псевдокод для $Q$ версии алгоритма приведен ниже:\n",
    "<img src=\"alg_ac.png\" style=\"width: 700px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1. Заполните пропуски для $V$ версии алгоритма Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "learning_rate = 0.0002\n",
    "gamma = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.data = []\n",
    "\n",
    "        # создаем сеть с двумя головами для pi и V (подсказка - смотрим на методы pi и v)\n",
    "        #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~        \n",
    "        raise NotImplementedError        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def pi(self, x, softmax_dim=0):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=softmax_dim)\n",
    "        return prob\n",
    "\n",
    "    def v(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "\n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "\n",
    "    def make_batch(self):\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_lst = [], [], [], [], []\n",
    "\n",
    "        for transition in self.data:\n",
    "            # собираем батч из сохранненых кортежей\n",
    "            # не забывайте сделать клиппинг вознаграждений!\n",
    "            #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~            \n",
    "            raise NotImplementedError            \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            \n",
    "            # не забываем про окончание эпизода\n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            done_lst.append([done_mask])\n",
    "\n",
    "        s_batch, a_batch, r_batch, s_prime_batch, done_batch = torch.tensor(s_lst, dtype=torch.float), torch.tensor(\n",
    "            a_lst), \\\n",
    "                                                               torch.tensor(r_lst, dtype=torch.float), torch.tensor(\n",
    "            s_prime_lst, dtype=torch.float), \\\n",
    "                                                               torch.tensor(done_lst, dtype=torch.float)\n",
    "        self.data = []\n",
    "        return s_batch, a_batch, r_batch, s_prime_batch, done_batch\n",
    "\n",
    "    def train_net(self):\n",
    "        # получаем батч\n",
    "        # s, a, r, s_prime, done =\n",
    "        #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~        \n",
    "        raise NotImplementedError        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "\n",
    "        # считаем td target(r + gamma ...) и td ошибку (td_target - v)\n",
    "        # td_target = \n",
    "        # delta = \n",
    "        #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~        \n",
    "        raise NotImplementedError        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "\n",
    "        # получаем распределение\n",
    "        pi = self.pi(s, softmax_dim=1)\n",
    "        pi_a = pi.gather(1, a)\n",
    "\n",
    "        # считаем td_loss (подсказка можно воспользоваться smooth_l1_loss) (не забыаем про лишние градиенты!)\n",
    "        # td_loss = \n",
    "        td_loss = F.smooth_l1_loss(self.v(s), td_target.detach())\n",
    "        # считаем loss актора (-pi_a) * delta (не забыаем про лишние градиенты!)\n",
    "        # a_loss =  \n",
    "        a_loss = -torch.log(pi_a) * delta.detach()\n",
    "\n",
    "        # вызываем backward и делаем шаг оптимизатора для каждого лосса\n",
    "        #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~        \n",
    "        raise NotImplementedError        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2. Подбираем гиперпараметры и строим графики. Как влияет клиппинг вознаграждений на итоговую сходимость?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    model = ActorCritic()\n",
    "    n_rollout = 5\n",
    "    print_interval = 20\n",
    "    score = 0.0\n",
    "\n",
    "    for n_epi in range(1000):\n",
    "        done = False\n",
    "        s = env.reset()\n",
    "        while not done:\n",
    "            for t in range(n_rollout):\n",
    "                prob = model.pi(torch.from_numpy(s).float())\n",
    "                m = Categorical(prob)\n",
    "                a = m.sample().item()\n",
    "                s_prime, r, done, info = env.step(a)\n",
    "                model.put_data((s, a, r, s_prime, done))\n",
    "\n",
    "                s = s_prime\n",
    "                score += r\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            model.train_net()\n",
    "\n",
    "        if n_epi % print_interval == 0 and n_epi != 0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score / print_interval))\n",
    "            score = 0.0\n",
    "    env.close()\n",
    "\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
