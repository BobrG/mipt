{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## Policy Gradient Algorithms: REINFORCE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u0422\u0435\u043e\u0440\u0435\u043c\u0430 \u043e \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0435 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0438 \u0441\u0432\u044f\u0437\u044b\u0432\u0430\u0435\u0442 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438  \u0438 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0441\u0430\u043c\u043e\u0439 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0438:\n", "\n", "$$J(\\theta) = \\mathbb{E}_\\pi [\\nabla_\\theta \\ln \\pi_\\theta(a \\vert s) Q^\\pi(s, a)]$$\n", "\u0415\u0441\u043b\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043c\u0435\u0442\u043e\u0434 \u041c\u043e\u043d\u0442\u0435-\u041a\u0430\u0440\u043b\u043e \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043d\u0435\u0441\u043c\u0435\u0449\u0435\u043d\u043d\u043e\u0439 \u043e\u0446\u0435\u043d\u043a\u0438 $Q^\\pi(s, a)$ \u043e\u0442\u0434\u0430\u0447\u0443 $R_t$, \u0442\u043e \u0442\u043e\u0433\u0434\u0430 \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442 \u043f\u0435\u0440\u0435\u0445\u043e\u0434 \u043a \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0443 REINFORCE \u0438 \u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 \u0432\u0435\u0441\u043e\u0432 \u0431\u0443\u0434\u0435\u0442 \u043e\u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043b\u044f\u0442\u044c\u0441\u044f \u043f\u043e \u043f\u0440\u0430\u0432\u0438\u043b\u0443:\n", "\n", "$$J(\\theta) = [R_t \\nabla_\\theta \\ln \\pi_\\theta(A_t \\vert S_t)]$$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"reinforce.png\">"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["# \u0438\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u043c\u044b\u0435 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438\n", "\n", "import gym\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torch.optim as optim\n", "from torch.distributions import Categorical"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \u0417\u0430\u0434\u0430\u043d\u0438\u0435 1. \u0417\u0430\u043f\u043e\u043b\u043d\u0438\u0442\u0435 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438 \u0432 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 REINFORCE"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["class Policy(nn.Module):\n", "    def __init__(self):\n", "        super(Policy, self).__init__()\n", "        # \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u0441\u0435\u0442\u044c \u0441 \u0434\u0432\u0443\u043c\u044f \u043f\u043e\u043b\u043d\u043e\u0441\u0432\u044f\u0437\u043d\u044b\u043c\u0438 \u0441\u043b\u043e\u044f\u043c\u0438 \u0438 \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c 4 (state input size)-> 128 -> 2 (actions)\n", "        self.fc1 = nn.Linear(4, 128)\n", "        self.fc2 = nn.Linear(128, 2)\n", "        # \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c Adam optimizer, \u0441 lr=0.0002\n", "        self.optimizer = optim.Adam(self.parameters(), lr=0.0002)\n", "\n", "    def forward(self, x):\n", "        # \u043a\u0430\u043a \u0434\u0430\u043d\u043d\u044b\u0435 \u0431\u0443\u0434\u0443\u0442 \u0438\u0434\u0442\u0438 \u043f\u043e \u0433\u0440\u0430\u0444\u0443 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439\n", "        x = F.relu(self.fc1(x))\n", "        x = F.softmax(self.fc2(x), dim=0)\n", "        return x\n", "\n", "    def train_net(self, rollout, gamma):\n", "        \"\"\"\n", "        \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043d\u0430\u0448\u0435\u0439 \u0441\u0435\u0442\u0438\n", "        :param self:\n", "        :param rollout: [(r, log(pi(s)))]\n", "        :gamma: \u0434\u0438\u0441\u043a\u043e\u043d\u0442\u0438\u0440\u0443\u044e\u0449\u0438\u0439 \u043c\u043d\u043e\u0436\u0438\u0442\u0435\u043b\u044c\n", "        :return:\n", "        \"\"\"\n", "        R = 0\n", "        # \u0438\u0434\u0435\u043c \u043f\u043e \u0441\u043f\u0438\u0441\u043a\u0443 \u0432 \u043e\u0431\u0440\u0430\u0442\u043d\u043e\u043c \u043f\u043e\u0440\u044f\u0434\u043a\u0435\n", "        for r, log_prob in rollout[::-1]:\n", "            # \u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0441\u0443\u043c\u043c\u0430\u0440\u043d\u043e\u0435 \u0434\u0438\u0441\u043a\u043e\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0435 \u0432\u043e\u0437\u043d\u0430\u0433\u0440\u0430\u0436\u0434\u0435\u043d\u0438\u0435\n", "            #~~~~~~~~ \u0412\u0430\u0448 \u043a\u043e\u0434 \u0437\u0434\u0435\u0441\u044c ~~~~~~~~~~~", "            \n", "            raise NotImplementedError", "            \n", "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n", "            \n", "\n", "            # \u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 loss \u0444\u0443\u043d\u043a\u0446\u0438\u0438\n", "            #~~~~~~~~ \u0412\u0430\u0448 \u043a\u043e\u0434 \u0437\u0434\u0435\u0441\u044c ~~~~~~~~~~~", "            \n", "            raise NotImplementedError", "            \n", "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n", "            \n", "\n", "            # \u043e\u0431\u043d\u0443\u043b\u044f\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0433\u043b\u0438 \u043d\u0430\u043a\u043e\u043f\u0438\u0442\u044c\u0441\u044f\n", "            self.optimizer.zero_grad()\n", "            # \u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0441\u0443\u043c\u043c\u0443 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043e\u0432 \u0442\u0435\u043d\u0437\u043e\u0440\u0430 loss\n", "            loss.backward()\n", "            # \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u0448\u0430\u0433 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438\n", "            self.optimizer.step()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043d\u0430\u0448\u0443 \u043c\u043e\u0434\u0435\u043b\u044c \u0438 \u0440\u0430\u0437\u0431\u0435\u0440\u0435\u043c \u0435\u0435 \u043c\u0435\u0442\u043e\u0434\u044b:"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["pi([ 0.03096003  0.04700809 -0.0375979  -0.03399649]) = tensor([0.5011, 0.4989], grad_fn=<SoftmaxBackward>)\n", "sampled action: 1\n"]}], "source": ["# \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\n", "e_pi = Policy()\n", "\n", "# \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u043e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u0435\n", "e_env = gym.make('CartPole-v1')\n", "\n", "# \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435\n", "state = e_env.reset()\n", "\n", "# \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0438 \u0434\u043b\u044f \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u044f\n", "prob = e_pi(torch.from_numpy(state).float())\n", "print(\"pi(\"+str(state)+ \") =\", prob)\n", "\n", "# \u0441\u044d\u043c\u043f\u043b\u0438\u0440\u0443\u0435\u043c \u0434\u0435\u0439\u0441\u0442\u0438\u0435, \u0441\u043e\u0433\u043b\u0430\u0441\u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u043c\u0443 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044e\n", "s_action = Categorical(prob).sample().item()\n", "print(\"sampled action:\", s_action)"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["def run():\n", "    env = gym.make('CartPole-v1')\n", "    pi = Policy()\n", "    score = 0.0\n", "    print_interval = 20\n", "    s = env.reset()\n", "    \n", "    for n_epi in range(10000):\n", "        s = env.reset()\n", "        rollout = []\n", "        for t in range(501):  # CartPole-v1 \u0431\u0443\u0434\u0435\u043c \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0435\u0440\u0432\u044b\u0435 500 \u0448\u0430\u0433\u043e\u0432 \u0432 \u043e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u0438\n", "            # \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0438\n", "            #~~~~~~~~ \u0412\u0430\u0448 \u043a\u043e\u0434 \u0437\u0434\u0435\u0441\u044c ~~~~~~~~~~~", "            \n", "            raise NotImplementedError", "            \n", "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n", "            \n", "            \n", "            # \u0441\u044d\u043c\u043f\u043b\u0438\u0440\u0443\u0435\u043c \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0435\n", "            #~~~~~~~~ \u0412\u0430\u0448 \u043a\u043e\u0434 \u0437\u0434\u0435\u0441\u044c ~~~~~~~~~~~", "            \n", "            raise NotImplementedError", "            \n", "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n", "            \n", "            \n", "            # \u0434\u0435\u043b\u0430\u0435\u043c \u0448\u0430\u0433 \u0432 \u043e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u0438\n", "            s_prime, r, done, info = env.step(a)\n", "            \n", "            # \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u0442\u0435\u043a\u0443\u0449\u0435\u0433\u043e \u0448\u0430\u0433\u0430\n", "            rollout.append((r, torch.log(prob[a])))\n", "            \n", "            s = s_prime\n", "            \n", "            # \u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\n", "            score += r\n", "            \n", "            if done:\n", "                break\n", "        \n", "        # \u043e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u043c \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u044e\n", "        pi.train_net(rollout, 0.98)\n", "        \n", "        # \u043f\u0435\u0447\u0430\u0442\u0430\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b\n", "        if n_epi % print_interval == 0 and n_epi != 0:\n", "            print(\"# of episode :{}, avg score : {}\".format(n_epi, score / print_interval))\n", "            score = 0.0\n", "    env.close()"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["# of episode :20, avg score : 31.15\n", "# of episode :40, avg score : 39.05\n", "# of episode :60, avg score : 40.6\n", "# of episode :80, avg score : 34.8\n", "# of episode :100, avg score : 44.0\n", "# of episode :120, avg score : 51.5\n", "# of episode :140, avg score : 67.75\n", "# of episode :160, avg score : 82.1\n", "# of episode :180, avg score : 109.1\n", "# of episode :200, avg score : 155.75\n"]}, {"ename": "KeyboardInterrupt", "evalue": "", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)", "\u001b[0;32m<ipython-input-5-ec9775ede022>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m<ipython-input-4-a0a1fb22f9c6>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# \u043e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u043c \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u044e\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.98\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# \u043f\u0435\u0447\u0430\u0442\u0430\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m<ipython-input-2-01cdd1f0cdf0>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(self, rollout, gamma)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# \u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0441\u0443\u043c\u043c\u0443 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043e\u0432 \u0442\u0435\u043d\u0437\u043e\u0440\u0430 loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;31m# \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u0448\u0430\u0433 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}], "source": ["run()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \u0417\u0430\u0434\u0430\u043d\u0438\u0435 2. \u0414\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435 \u043a\u043e\u0434, \u0434\u043b\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u0433\u0440\u0430\u0444\u0438\u043a\u0430 \u0441\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["\u041a\u0430\u043a\u0438\u0435 \u0432\u044b\u0432\u043e\u0434\u044b \u043c\u043e\u0436\u043d\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0438\u0437 \u044d\u0442\u043e\u0433\u043e \u0433\u0440\u0430\u0444\u0438\u043a\u0430?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### \u0414\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u043c\u0430\u0442\u0435\u0440\u0438\u0430\u043b\u044b\n", "https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.3"}}, "nbformat": 4, "nbformat_minor": 2}