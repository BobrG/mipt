{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из наиболее популярных алгоритм обучения на основе временных различий является Q-обучение. Агент, который принимает решения на основе $Q$-функции, не требует модель для обучения и выбора действий, т.е. такой агент также свободен от модели (model-free), как и TD-агент. Уравнение Беллмана для значения Q-функции в равновесии записывается как:\n",
    "\n",
    "$$Q(s,a)=r(s)+\\gamma\\sum_s'T(s,a,s')\\max_{a'}Q(a',s')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уравнение для итерационного обновления значений Q-функции выглядит следующим образом:\n",
    "$$Q(s,a)\\leftarrow Q(s,a)+\\alpha(r(s)+\\gamma\\max_{a'}Q(a',s') - Q(s,a)).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"q.png\" caption=\"Алгоритм Q-обучения\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1\n",
    "Оформим этот алгоритм в виде класса QLearningAgent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class QLearningAgent():\n",
    "    \"\"\"\n",
    "    Q-Learning агент\n",
    "\n",
    "    Замечание: избегайте прямое использование \n",
    "    self._q_values, для этого определены \n",
    "    функции: get_q_value, set_q_value\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha, epsilon, discount, \n",
    "                 get_legal_actions):\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._q_values = \\\n",
    "            defaultdict(lambda: defaultdict(lambda: 0))  \n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        return self._q_values[state][action]\n",
    "\n",
    "    def set_q_value(self, state, action, value):\n",
    "        self._q_values[state][action] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим нашему агенту возможность вычислять оценки $V$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(self, state):\n",
    "    \"\"\"\n",
    "      Возвращает значение функции полезности, \n",
    "      рассчитанной по Q[state, action], \n",
    "    \"\"\"\n",
    "    possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "    # value = \n",
    "    #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~    \n",
    "    raise NotImplementedError    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    return value\n",
    "\n",
    "QLearningAgent.get_value = get_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стратегия нашего агента будет заключаться в выборе лучшего действия, в соответствии с оценками $Q$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(self, state):\n",
    "    \"\"\"\n",
    "      Выбирает лучшее действие, согласно стратегии.\n",
    "    \"\"\"\n",
    "    possible_actions = self.get_legal_actions(state)\n",
    "    \n",
    "    # выбираем лучшее действие, согласно стратегии\n",
    "    #best_action = \n",
    "    #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~    \n",
    "    raise NotImplementedError    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    return best_action\n",
    "\n",
    "QLearningAgent.get_policy = get_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для конкретной ситуации мы будем выбирать действие, используя $\\epsilon$-жадный подход:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(self, state):\n",
    "    \"\"\"\n",
    "      Выбирает действие, предпринимаемое в данном \n",
    "      состоянии, включая исследование (eps greedy)\n",
    "      С вероятностью self.epsilon берем случайное \n",
    "      действие, иначе действие согласно стратегии \n",
    "      (self.get_policy)\n",
    "    \"\"\"\n",
    "    possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "    # выбираем действие, используя eps-greedy подход\n",
    "    # action = \n",
    "    #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~    \n",
    "    raise NotImplementedError    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    return action\n",
    "\n",
    "QLearningAgent.get_action = get_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, state, action, next_state, reward):\n",
    "    \"\"\"\n",
    "      функция Q-обновления \n",
    "    \"\"\"\n",
    "    # выполняем Q-обновление, \n",
    "    # используем методы getQValue и setQValue\n",
    "    #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~    \n",
    "    raise NotImplementedError    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "QLearningAgent.update = update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестируем нашего агента на задаче Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_train(env, agent, t_max=10**4):\n",
    "    \"\"\"функция запускает полную игру,\n",
    "    используя стратегию агента (agent.get_action(s))\n",
    "    выполняет обновление агента (agent.update(...))\n",
    "    и возвращает общее вознаграждение\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        # выбираем действие\n",
    "        # a = \n",
    "        #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~        \n",
    "        raise NotImplementedError        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        \n",
    "        # выполняем обновление стратегии\n",
    "        # agent.update()\n",
    "        #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~        \n",
    "        raise NotImplementedError        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "        s = next_s\n",
    "        total_reward +=r\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "agent = QLearningAgent(alpha=0.5, epsilon=0.1,\n",
    "                       discount=0.9,\n",
    "                       get_legal_actions=lambda s: range(\n",
    "                           n_actions))\n",
    "\n",
    "assert 'get_policy' in dir(agent)\n",
    "rewards = []\n",
    "for i in range(5000):\n",
    "    rewards.append(play_and_train(env, agent))\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        clear_output(True)\n",
    "        print('eps =', agent.epsilon,\n",
    "              'mean reward =', np.mean(rewards[-10:]))\n",
    "        print(\"alpha=\", agent.alpha)\n",
    "        plt.plot(rewards)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2\n",
    "\n",
    "Рассмотрим задачу CartPole. Это окружение имеет непрерывное множество состояний - попробуем их сгруппировать. Для этого попробуем использовать `round(x, n_digits)` для округления действительных чисел."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"начальное состояние: %s\" % (env.reset()))\n",
    "plt.imshow(env.render('rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим распределение наблюдений -- сыграем несколько эпизодов и запишем состояния."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = []\n",
    "for _ in range(1000):\n",
    "    all_states.append(env.reset())\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        s, r, done, _ = env.step(action)\n",
    "        all_states.append(s)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "all_states = np.array(all_states)\n",
    "\n",
    "for obs_i in range(env.observation_space.shape[0]):\n",
    "    plt.hist(all_states[:, obs_i], bins=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим обертку для окружения, которая проводить бинаризацию состояний:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.core import ObservationWrapper\n",
    "\n",
    "\n",
    "class Binarizer(ObservationWrapper):\n",
    "\n",
    "    def observation(self, state):\n",
    "\n",
    "        # state = <round state to some amount digits.>\n",
    "        # подсказка: используйте round(x,n_digits)\n",
    "        # необходимо выбрать разные параметры n_digits\n",
    "        # для каждой размерности\n",
    "        #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~        \n",
    "        raise NotImplementedError        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "        return tuple(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Binarizer(gym.make(\"CartPole-v0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_states = []\n",
    "for _ in range(1000):\n",
    "    all_states.append(env.reset())\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        s, r, done, _ = env.step(action)\n",
    "        all_states.append(s)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "all_states = np.array(all_states)\n",
    "\n",
    "for obs_i in range(env.observation_space.shape[0]):\n",
    "\n",
    "    plt.hist(all_states[:, obs_i], bins=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим нашего Q-агента:\n",
    "Если бинаризация слишком грубая, агент может не обучаться.\n",
    "Если бинаризация слишком точная, сходимость процесса обучения может занять слишком большое количество шагов.\n",
    "Размерность в $10^3$-$10^4$ состояний - оптимальная.\n",
    " Хорошим является агент, получающий вознаграждение >= 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QLearningAgent(alpha=0.7, epsilon=0.25,\n",
    "                       discount=0.99,\n",
    "                       get_legal_actions=lambda s: range(\n",
    "                           n_actions))\n",
    "\n",
    "rewards = []\n",
    "for i in range(10001):\n",
    "    rewards.append(play_and_train(env, agent))\n",
    "\n",
    "    # опционально: добавьте уменьшение eps\n",
    "    if i % 1000 == 0:\n",
    "        clear_output(True)\n",
    "        print('eps =', agent.epsilon, 'mean reward =',\n",
    "              np.mean(rewards[-100:]))\n",
    "        plt.plot(rewards)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
