{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# PPO (Proximal Policy Optimization)\n", "\n", "<img src=\"ppo1.png\">\n", "<img src=\"ppo2.png\">"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["import gym\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torch.optim as optimizers\n", "from torch.distributions import Categorical"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Задание 1. Заполните пропуски в методе __train_net__"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["class PPO(nn.Module):\n", "    def __init__(self, learning_rate, gamma, eps_clip):\n", "        super(PPO, self).__init__()\n", "        self.data = []\n", "\n", "        self.fc1 = nn.Linear(4, 256)\n", "        self.fc_pi = nn.Linear(256, 2)\n", "        self.fc_v = nn.Linear(256, 1)\n", "        self.optimizer = optimizers.Adam(self.parameters(), lr=learning_rate)\n", "        self.gamma = gamma\n", "        self.eps_clip = eps_clip\n", "\n", "    def pi(self, x, softmax_dim=0):\n", "        \"\"\"\n", "        define computation graph for pi\n", "        :param x: input\n", "        :param softmax_dim:\n", "        :return:\n", "        \"\"\"\n", "        x = F.relu(self.fc1(x))\n", "        x = self.fc_pi(x)\n", "        prob = F.softmax(x, dim=softmax_dim)\n", "        return prob\n", "\n", "    def v(self, x):\n", "        \"\"\"\n", "        define computation graph for v\n", "        :param x: вход\n", "        :return:\n", "        \"\"\"\n", "        x = F.relu(self.fc1(x))\n", "        v = self.fc_v(x)\n", "        return v\n", "\n", "    def put_data(self, transition):\n", "        \"\"\"\n", "        memorizing transitions\n", "        :param transition:\n", "        :return:\n", "        \"\"\"\n", "        self.data.append(transition)\n", "\n", "    def make_batch(self):\n", "        \"\"\"\n", "        we have already seen, almost all the errors that you can get creating batches,\n", "        so don't bother yourself coding this :)\n", "        :return:\n", "        \"\"\"\n", "        s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n", "\n", "        for transition in self.data:\n", "            s, a, r, s_prime, prob_a, done = transition\n", "\n", "            s_lst.append(s)\n", "            a_lst.append([a])\n", "            r_lst.append([r])\n", "            s_prime_lst.append(s_prime)\n", "            prob_a_lst.append([prob_a])\n", "            done_mask = 0 if done else 1\n", "            done_lst.append([done_mask])\n", "\n", "        s = torch.tensor(s_lst, dtype=torch.float)\n", "        a = torch.tensor(a_lst)\n", "        r = torch.tensor(r_lst)\n", "        s_prime = torch.tensor(s_prime_lst, dtype=torch.float)\n", "        done_mask = torch.tensor(done_lst, dtype=torch.float)\n", "        prob_a = torch.tensor(prob_a_lst)\n", "        self.data = []\n", "        return s, a, r, s_prime, done_mask, prob_a\n", "\n", "    def train_net(self, epochs):\n", "        \"\"\"\n", "        training function\n", "        :param epochs: number of epochs\n", "        :return:\n", "        \"\"\"\n", "        s, a, r, s_prime, done_mask, prob_a = self.make_batch()\n", "\n", "        for i in range(epochs):\n", "            # compute td\n", "            # td_target =\n", "            ########## Solution ############\n", "            td_target = r + self.gamma * self.v(s_prime) * done_mask\n", "            ################################\n", "            delta = td_target - self.v(s)\n", "            delta = delta.detach().numpy()\n", "\n", "            advantage_lst = []\n", "            advantage = 0.0\n", "            # compute advantage_lst (pay attention to the order)\n", "            ########## Solution ############\n", "            for delta_t in delta[::-1]:\n", "                advantage = self.gamma * advantage + delta_t[0]\n", "                advantage_lst.append([advantage])\n", "            ################################\n", "            advantage_lst.reverse()\n", "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n", "\n", "            # getting pi_a\n", "            pi = self.pi(s, softmax_dim=1)\n", "            pi_a = pi.gather(1, a)\n", "\n", "            # computes first part of surrogate function\n", "            # surr1 =\n", "            ########## Solution ############\n", "            surr1 = torch.exp(torch.log(pi_a) - torch.log(prob_a)) * advantage\n", "            ################################\n", "\n", "            # second part\n", "            # surr2 = torch.clamp(torch.exp(torch.log(pi_a) - torch.log(prob_a)), TODO, TODO) * advantage\n", "            ########## Solution ############\n", "            surr2 = torch.clamp(torch.exp(torch.log(pi_a) - torch.log(prob_a)), 1 - self.eps_clip,\n", "                                1 + self.eps_clip) * advantage\n", "            ################################\n", "            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s), td_target.detach())\n", "\n", "            # make optimizer step (just copy/paste it from previous seminar :) )\n", "            ########## Solution ############\n", "            self.optimizer.zero_grad()\n", "            loss.mean().backward()\n", "            self.optimizer.step()\n", "            ################################"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["class RewardScalingWrapper(gym.RewardWrapper):\n", "    def __init__(self, env, scale):\n", "        super().__init__(env)\n", "        self.scale = scale\n", "\n", "    def reward(self, reward):\n", "        return reward * self.scale\n", "\n", "\n", "def run(episodes_number=1001, print_interval=20, reward_scale_f=0.01, epochs=3, time_horizon=20,\n", "       learning_rate=0.0005, gamma=0.98, eps_clip=0.1):\n", "    # adding reward scaling via wrapper\n", "    env = RewardScalingWrapper(gym.make('CartPole-v1'), scale=reward_scale_f)\n", "    model = PPO(learning_rate, gamma, eps_clip)\n", "    score = 0.0\n", "\n", "    for n_epi in range(episodes_number):\n", "        s = env.reset()\n", "        done = False\n", "        while not done:\n", "            for t in range(time_horizon):\n", "                # noinspection PyUnresolvedReferences\n", "                prob = model.pi(torch.from_numpy(s).float())\n", "                m = Categorical(prob)\n", "                a = m.sample().item()\n", "                s_prime, r, done, info = env.step(a)\n", "\n", "                # noinspection PyUnresolvedReferences\n", "                model.put_data((s, a, r, s_prime, prob[a].item(), done))\n", "                s = s_prime\n", "\n", "                score += r\n", "                if done:\n", "                    break\n", "\n", "            # noinspection PyUnresolvedReferences\n", "            model.train_net(epochs)\n", "\n", "        if n_epi % print_interval == 0 and n_epi != 0:\n", "            print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score / print_interval / reward_scale_f))\n", "            score = 0.0\n", "\n", "    env.close()"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["# of episode :20, avg score : 19.3\n", "# of episode :40, avg score : 28.1\n", "# of episode :60, avg score : 44.6\n", "# of episode :80, avg score : 42.8\n", "# of episode :100, avg score : 54.9\n", "# of episode :120, avg score : 63.0\n", "# of episode :140, avg score : 64.3\n", "# of episode :160, avg score : 69.6\n", "# of episode :180, avg score : 72.3\n", "# of episode :200, avg score : 78.0\n", "# of episode :220, avg score : 84.2\n", "# of episode :240, avg score : 100.5\n", "# of episode :260, avg score : 108.0\n", "# of episode :280, avg score : 179.4\n", "# of episode :300, avg score : 213.4\n", "# of episode :320, avg score : 167.8\n", "# of episode :340, avg score : 272.9\n", "# of episode :360, avg score : 279.8\n", "# of episode :380, avg score : 292.2\n", "# of episode :400, avg score : 290.7\n", "# of episode :420, avg score : 270.8\n", "# of episode :440, avg score : 290.2\n", "# of episode :460, avg score : 320.8\n", "# of episode :480, avg score : 265.9\n", "# of episode :500, avg score : 437.6\n", "# of episode :520, avg score : 400.0\n", "# of episode :540, avg score : 459.5\n", "# of episode :560, avg score : 450.7\n", "# of episode :580, avg score : 311.1\n", "# of episode :600, avg score : 432.4\n", "# of episode :620, avg score : 469.8\n", "# of episode :640, avg score : 495.9\n", "# of episode :660, avg score : 494.0\n", "# of episode :680, avg score : 485.0\n", "# of episode :700, avg score : 484.6\n", "# of episode :720, avg score : 458.0\n", "# of episode :740, avg score : 496.2\n", "# of episode :760, avg score : 472.9\n", "# of episode :780, avg score : 495.8\n", "# of episode :800, avg score : 481.8\n", "# of episode :820, avg score : 467.7\n", "# of episode :840, avg score : 493.2\n", "# of episode :860, avg score : 500.0\n", "# of episode :880, avg score : 500.0\n", "# of episode :900, avg score : 499.2\n", "# of episode :920, avg score : 492.9\n", "# of episode :940, avg score : 498.6\n", "# of episode :960, avg score : 418.3\n", "# of episode :980, avg score : 500.0\n", "# of episode :1000, avg score : 500.0\n"]}], "source": ["run(learning_rate=0.0003, time_horizon=100)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Задание 2. Проверьте работу алгоритма с различными гиперпараметрами (epochs, time_horizon, learning_rate, eps_clip)  и постройте графики"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Бонус. Добавьте entropy exploration bonus\n", "<img src=\"entropy.png\">"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Дополнительные материалы\n", "[Лекция 8. Оптимизация градиента стратегии](https://yadi.sk/i/gTNlK0m4_A1U7Q)\n", "\n", "[Bootcamp 2017 - Shulman - NPG, TRPO, PPO](https://yadi.sk/i/E0Ua9lmEzUPsrQ)\n", "\n", "[Openai baselines ppo](https://openai.com/blog/openai-baselines-ppo/)\n", "\n", "[Proximal policy optimization with sonic the hedgehog 2](https://towardsdatascience.com/proximal-policy-optimization-ppo-with-sonic-the-hedgehog-2-and-3-c9c21dbed5e)\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.5rc1"}}, "nbformat": 4, "nbformat_minor": 2}