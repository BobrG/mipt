{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сверточные нейронные сети в Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основа современных глубоких сетей - это два слоя Convolution (сверточный) и Max pooling (взятие максимума)\n",
    "\n",
    "Convolution:\n",
    "<img src=\"3.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max pooling:\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/Max_pooling.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1\n",
    "\n",
    "Создадим следующую архитектуру:\n",
    "\n",
    "![Architecture](https://www.tensorflow.org/images/mnist_deep.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим ее на стандартном датасете MNIST, посмотрим точность на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  for i in range(20000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i % 100 == 0:\n",
    "      #### Каждый 100 шагов мы считаем точность нашей модели keep_probw=1\n",
    "    ### Здесь мы обучаем нашу модель на подвыборке! keep_probw=0.5\n",
    "\n",
    "  ### Выводим точность на тесте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 \n",
    "\n",
    "А теперь поработаем с более сложным датасетом и восопльзуемся еще одной библиотекой-оболочкой - [Keras](https://keras.io)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 содержит 32x32 цветных изображения 10 классов: __airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck__:\n",
    "<img src=\"cifar10.jpg\" style=\"width:80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print(\"Train samples:\", x_train.shape, y_train.shape)\n",
    "print(\"Test samples:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "cifar10_classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \n",
    "                   \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "# show random images from train\n",
    "cols = 8\n",
    "rows = 2\n",
    "fig = plt.figure(figsize=(2 * cols - 1, 2.5 * rows - 1))\n",
    "for i in range(cols):\n",
    "    for j in range(rows):\n",
    "        random_index = np.random.randint(0, len(y_train))\n",
    "        ax = fig.add_subplot(rows, cols, i * rows + j + 1)\n",
    "        ax.grid('off')\n",
    "        ax.axis('off')\n",
    "        ax.imshow(x_train[random_index, :])\n",
    "        ax.set_title(cifar10_classes[y_train[random_index, 0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного предобработаем наши данные:\n",
    "\n",
    "Нормализуем картинки: $$x_{norm} = \\frac{x}{255} - 0.5$$\n",
    "и конвертируем метки классов в категориаьные (см.  __keras.utils__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs\n",
    "x_train2 = ### Ваш код\n",
    "x_test2 =  ### Ваш код\n",
    "\n",
    "# convert class labels to one-hot encoded, should have shape (?, NUM_CLASSES)\n",
    "y_train2 =  ### Ваш код\n",
    "y_test2 =  ### Ваш код"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строим сеть с использованием Keras:\n",
    "\n",
    "- [Conv2D](https://keras.io/layers/convolutional/#conv2d) - сверточный слой:\n",
    "- [MaxPooling2D](https://keras.io/layers/pooling/#maxpooling2d) - взятие максимума.\n",
    "- [Flatten](https://keras.io/layers/core/#flatten) - слой вытяивания в один вектор.\n",
    "- [Dense](https://keras.io/layers/core/#dense) - полносвязный слой.\n",
    "- [Activation](https://keras.io/layers/core/#activation) - применение функции активации.\n",
    "- [LeakyReLU](https://keras.io/layers/advanced-activations/#leakyrelu) - активация ReLU с утечкой.\n",
    "- [Dropout](https://keras.io/layers/core/#dropout) - прореживание."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша задача - определить модель, принимающую на вход тензор  __(None, 32, 32, 3)__ и предсказывающую выходной тензор __(None, 10)__, отвечающий за вероятности классов. __None__ стоит на месте размерности подвыборки.\n",
    "\n",
    "Мы будем строить простую архитектуру вида стакан:\n",
    "\n",
    "```python\n",
    "model = Sequential()  # start feed-forward model definition\n",
    "model.add(Conv2D(..., input_shape=(32, 32, 3)))  # first layer needs to define \"input_shape\"\n",
    "\n",
    "...  # here comes a bunch of convolutional, pooling and dropout layers\n",
    "\n",
    "model.add(Dense(NUM_CLASSES))  # the last layer with neuron for each class\n",
    "model.add(Activation(\"softmax\"))  # output probabilities\n",
    "```\n",
    "\n",
    "Складываем вместе __4__ сверточныйх слоя с размером ядра __(3, 3)__ и с возрастающим количеством фильтров __(16, 32, 32, 64)__, используем \"same\" смещение (padding).\n",
    "\n",
    "Добавляем слои максимизации с окном __2x2__  послде каждых двух сверточных слоев (conv-conv-pool схема).\n",
    "\n",
    "Используем __LeakyReLU__ активацию с параметром утечки __0.1__ для всех слоев, где она применима:\n",
    "```python\n",
    "model.add(LeakyReLU(0.1))\n",
    "```\n",
    "\n",
    "Добавляем полносвязный слой с __256__ нейронами и второй выходной с __10__ нейронами, соответствующими классам. Не забудьте использовать __Flatten__ слой перед тем, как добавите полносвязные!\n",
    "\n",
    "Наконец, добавляем слой __Dropout__ после каждого слоя максимизации с параметром прореживания (__0.25__), а после первого полносвязаного - с параметром (__0.5__)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary building blocks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "def make_model():\n",
    "    \"\"\"\n",
    "    Define your model architecture here.\n",
    "    Returns `Sequential` model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    ### Ваш код\n",
    "    \n",
    "    model.add(Dense(NUM_CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_utils\n",
    "from keras_utils import reset_tf_session\n",
    "\n",
    "# describe model\n",
    "s = reset_tf_session()\n",
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь насталая очередь учить и тестировать нашу модель. В процессе обучения мы хотим наблюдать изменения в функции потерь на валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 5e-3  # initial learning rate\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "s = reset_tf_session()  # clear default graph\n",
    "# don't call K.set_learning_phase() !!! (otherwise will enable dropout in train/test simultaneously)\n",
    "model = make_model()  # define our model\n",
    "\n",
    "# prepare model for fitting (loss, optimizer, etc)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',  # we train 10-way classification\n",
    "    optimizer=keras.optimizers.adamax(lr=INIT_LR),  # for SGD\n",
    "    metrics=['accuracy']  # report accuracy during training\n",
    ")\n",
    "\n",
    "# scheduler of learning rate (decay with epochs)\n",
    "def lr_scheduler(epoch):\n",
    "    return INIT_LR * 0.9 ** epoch\n",
    "\n",
    "# callback for printing of actual learning rate used by optimizer\n",
    "class LrHistory(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        print(\"Learning rate:\", K.get_value(model.optimizer.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will save model checkpoints to continue training in case of kernel death\n",
    "model_filename = 'cifar.{0:03d}.hdf5'\n",
    "last_finished_epoch = None\n",
    "\n",
    "#### uncomment below to continue training from model checkpoint\n",
    "#### fill `last_finished_epoch` with your latest finished epoch\n",
    "# from keras.models import load_model\n",
    "# s = reset_tf_session()\n",
    "# last_finished_epoch = 7\n",
    "# model = load_model(model_filename.format(last_finished_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit(\n",
    "    x_train2, y_train2,  # prepared data\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[keras.callbacks.LearningRateScheduler(lr_scheduler), \n",
    "               LrHistory(), \n",
    "               keras_utils.TqdmProgressCallback(),\n",
    "               keras_utils.ModelSaveCallback(model_filename)],\n",
    "    validation_data=(x_test2, y_test2),\n",
    "    shuffle=True,\n",
    "    verbose=0,\n",
    "    initial_epoch=last_finished_epoch or 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights to file\n",
    "#model.save_weights(\"weights.h5\")\n",
    "# load weights from file (can call without model.fit)\n",
    "#model.load_weights(\"weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строим матрицу ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make test predictions\n",
    "y_pred_test = model.predict_proba(x_test2)\n",
    "y_pred_test_classes = np.argmax(y_pred_test, axis=1)\n",
    "y_pred_test_max_probas = np.max(y_pred_test, axis=1)\n",
    "\n",
    "# confusion matrix and accuracy\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.title('Confusion matrix', fontsize=16)\n",
    "plt.imshow(confusion_matrix(y_test, y_pred_test_classes))\n",
    "plt.xticks(np.arange(10), cifar10_classes, rotation=45, fontsize=12)\n",
    "plt.yticks(np.arange(10), cifar10_classes, fontsize=12)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, y_pred_test_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем несколько предсказаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect preditions\n",
    "cols = 8\n",
    "rows = 2\n",
    "fig = plt.figure(figsize=(2 * cols - 1, 3 * rows - 1))\n",
    "for i in range(cols):\n",
    "    for j in range(rows):\n",
    "        random_index = np.random.randint(0, len(y_test))\n",
    "        ax = fig.add_subplot(rows, cols, i * rows + j + 1)\n",
    "        ax.grid('off')\n",
    "        ax.axis('off')\n",
    "        ax.imshow(x_test[random_index, :])\n",
    "        pred_label = cifar10_classes[y_pred_test_classes[random_index]]\n",
    "        pred_proba = y_pred_test_max_probas[random_index]\n",
    "        true_label = cifar10_classes[y_test[random_index, 0]]\n",
    "        ax.set_title(\"pred: {}\\nscore: {:.3}\\ntrue: {}\".format(\n",
    "               pred_label, pred_proba, true_label\n",
    "        ))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
