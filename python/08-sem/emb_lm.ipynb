{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторная модель текста в NLTK\n",
    "### Задание 1\n",
    "Возьмем файл alice.txt с прошлого семинара. Мы уже научились разбивать его на токены и удалять стоп-слова.\n",
    "Используя nltk.FreqDist, выведите 20 самых часто встречающихся слов в тексте (не удаляя стоп символы).\n",
    "\n",
    "Ответьте на вопросы:\n",
    "\n",
    "    Какое слово стоит на 1 месте?\n",
    "    Сколько раз встречается слово, стоящее на 20 месте?\n",
    "    Сколько не стоп-слов среди первых десяти?\n",
    "    Сколько раз встречается слово \"крокет\"? (понятно, что оно не входит в top20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# d1 = nltk.FreqDist(l1)\n",
    "# d1.most_common(10)\n",
    "\n",
    "# d3['...'], d3.freq('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2\n",
    "Избавьтесь от стоп-слов с помощью списка stopwords из nltk.corpus. Используйте pymorphy2 или Mystem, чтобы привести слова к нормальной форме. Ответьте на вопросы:\n",
    "\n",
    "    Какое слово стоит на 1 месте?\n",
    "    Сколько раз встречается слово, стоящее на 20 месте?\n",
    "    Остались ли какие-то стоп-слова среди первых 10? 20?\n",
    "    Сколько раз встречается слово \"крокет\"? Почему результат изменился?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# morph = pymorphy2.MorphAnalyzer()\n",
    "# l3 = [morph.parse(token)[0].normal_form for token in l1 if not token in stopwords.words('russian')]\n",
    "\n",
    "# d3['...'], d3.freq('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте пострим кумулятивный график частот слов. Как он выглядит и почему так? Попробуйте поменять количество слов, какие тенденции вы видите?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d3.plot (50, cumulative = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3\n",
    "\n",
    "Найдите распределение длин слов в тексте, выведите top5 и ответьте на вопросы:\n",
    "\n",
    "    Сколько всего в данном тексте вариантов длин слов (вместе со стоп-словами)?\n",
    "    Слов какой длины в тексте больше всего?\n",
    "    Какую долю занимают слова этой длины?\n",
    "    Сколько слов длины 1?\n",
    "\n",
    "Выведите в алфавитном поредке длинные слова (длиннее 15 символов).\n",
    "\n",
    "    Сколько их?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fdist = nltk.FreqDist(len(w) for w in l1)  \n",
    "# print(fdist)  \n",
    "# print (fdist.most_common(5))\n",
    "# print (fdist.freq(fdist.max()))\n",
    "\n",
    "# long_words = [w for w in l1 if len(w) > 10]\n",
    "# sorted(long_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4\n",
    "\n",
    "Превратите строку в NLTK-текст и проверьте, как работают спецфункции, на собственных примерах.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text = nltk.Text(l1)\n",
    "# print (text.concordance(\".....\")) \n",
    "# print (text.similar('......'))\n",
    "# print (text.common_contexts([\".....\", \"......\"]))\n",
    "# print (text.collocations(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эмбеддинги\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(1228)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачиваем куски датасета твитов ([источник](http://study.mokoron.com/)): [положительные](https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0), [отрицательные](https://www.dropbox.com/s/ilkte35m35l38mr/negative.sql)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv?dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем лемматизированные статьи без стоп-слов и создаем массив текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import re\n",
    "\n",
    "\n",
    "m = Mystem()\n",
    "\n",
    "\n",
    "regex = re.compile(\"[А-Яа-я:=!\\)\\()A-z\\_\\%/|]+\")\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    try:\n",
    "        return \" \".join(regex.findall(text))\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize(text, mystem=m):\n",
    "    try:\n",
    "        return \"\".join(m.lemmatize(text)).strip()  \n",
    "    except:\n",
    "        return \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_neg = pd.read_csv(\"negative.csv?dl=0\", sep=';', header = None, usecols = [3])\n",
    "df_pos = pd.read_csv(\"positive.csv?dl=0\", sep=';', header = None, usecols = [3])\n",
    "df_neg['sent'] = 'neg'\n",
    "df_pos['sent'] = 'pos'\n",
    "df_pos['text'] = df_pos[3]\n",
    "df_neg['text'] = df_neg[3]\n",
    "df = pd.concat([df_neg, df_pos])\n",
    "df = df[['text', 'sent']]\n",
    "%time df.text = df.text.apply(words_only)\n",
    "%time df.text = df.text.apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [df.text.iloc[i].split() for i in range(len(df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели в gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(texts, size=100, window=5, min_count=5, workers=4)\n",
    "model.save(\"word2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# здесь можно посмотреть все слова, к которым мы имеем вектора\n",
    "# model.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем обученную модель (для скорости):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# model = Word2Vec.load(\"sent_w2v.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно воспользjваться функциями поиска похожих / непохожих слов на данное\n",
    "\n",
    "    model.most_similar\n",
    "    model.doesnt_match\n",
    "    etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.most_similar(\"корпоратив\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"король\",\"добрый\"], negative=[\"хороший\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.doesnt_match(\"борщ сметана макароны пирожок котлета\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5\n",
    "Выделите top 100 самых частотных слов и получите для них только что обученные вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "fd = FreqDist()\n",
    "# your code is hear\n",
    "\n",
    "top_words = # your code is hear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model['рабочий']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_words_vec = # your code is hear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем вектора. Для этого воспользуемся методом снижения размерности t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "top_words_tsne = tsne.fit_transform(top_words_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "p = figure(tools=\"pan,wheel_zoom,reset,save\",\n",
    "           toolbar_location=\"above\",\n",
    "           title=\"word2vec T-SNE for most common words\")\n",
    "\n",
    "source = ColumnDataSource(data=dict(x1=top_words_tsne[:,0],\n",
    "                                    x2=top_words_tsne[:,1],\n",
    "                                    names=top_words))\n",
    "\n",
    "p.scatter(x=\"x1\", y=\"x2\", size=8, source=source)\n",
    "\n",
    "labels = LabelSet(x=\"x1\", y=\"x2\", text=\"names\", y_offset=6,\n",
    "                  text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                  source=source, text_align='center')\n",
    "p.add_layout(labels)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нарисуем облако слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import *\n",
    "word_freq = [i for i in fd.most_common(100)]\n",
    "wd = WordCloud(background_color = 'white')\n",
    "wd.generate_from_frequencies(dict(word_freq))\n",
    "plt.figure()\n",
    "plt.imshow(wd, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кластеризация слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(top_words_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Языковые модели\n",
    "\n",
    "Какое слово в последовательности вероятнее: \n",
    "\n",
    "Поезд прибыл на\n",
    "* вокзал\n",
    "* север\n",
    "\n",
    "Какая последовательность вероятнее:\n",
    "* Вокзал прибыл поезд на\n",
    "* Поезд прибыл на вокзал\n",
    "\n",
    "Языковая модель [language model, LM]  позволяет оценить вероятность следующего слова в последовательности  $P(w_n | w_1, \\ldots, w_{n-1})$ и оценить вероятность всей последовательности слов $P(w_1, \\ldots, w_n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Приложения:\n",
    "* Задачи, в которых нужно обработать сложный и зашумленный вход: распознавание речи, распознавание сканированных и рукописных текстов;\n",
    "* Исправление опечаток\n",
    "* Машинный перевод\n",
    "* Подсказка при наборе "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Виды моделей:\n",
    "* Счетные модели\n",
    "    * цепи Маркова\n",
    "* Нейронные модели, обычно реккурентные нейронные сети с LSTM/GRU\n",
    "* seq2seq архитектуры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Модель $n$-грам\n",
    "\n",
    "Пусть $w_{1:n}=w_1,\\ldots,w_m$ – последовательность слов.\n",
    "\n",
    "Цепное правило:\n",
    "$ P(w_{1:m}) = P(w_1) P(w_2 | w_1) P(w_3 | w_{1:2}) \\ldots P(w_m | w_{1:m-1}) = \\prod_{k=1}^{m} P(w_k | w_{1:k-1}) $\n",
    "\n",
    "Но оценить $P(w_k | w_{1:k-1})$ не легче! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Переходим к $n$-грамам: $P(w_{i+1} | w_{1:i}) \\approx P(w_{i+1} | w_{i-n:i})  $ , то есть, учитываем $n-1$ предыдущее слово. \n",
    "\n",
    "Модель\n",
    "* униграм: $P(w_k)$\n",
    "* биграм: $P(w_k | w_{k-1})$\n",
    "* триграм: $P(w_k | w_{k-1} w_{k-2})$\n",
    "\n",
    "\n",
    "Т.е. используем Марковские допущения о длине запоминаемой цепочки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Вероятность следующего слова в последовательности: $ P(w_{i+1} | w_{1:i}) \\approx P(w_{i-n:i}) $\n",
    "* Вероятность всей последовательности слов: $P(w_{1:n}) = \\prod_{k=1}^l P(w_k | w_{k-n+1: k-1}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Качество модели  $n$-грам\n",
    "\n",
    "Перплексия: насколько хорошо модель предсказывает выборку. Чем ниже значение перплексии, тем лучше.\n",
    "\n",
    "$PP(\\texttt{LM}) = 2 ^ {-\\frac{1}{m} \\log_2 \\texttt{LM} (w_i | w_{1:i-1})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Счетные языковые модели\n",
    "\n",
    "### ММП оценки вероятностей\n",
    "$ P_{MLE}(w_k | w_{k-n+1:k-1}) = \\frac{\\texttt{count}(w_{k-n+1:k-1} w_k )}{\\texttt{count}(w_{k-n+1:k-1} )} $\n",
    "\n",
    "В модели биграм:\n",
    "\n",
    "$ P_{MLE}(w_k | w_{k-1}) = \\frac{\\texttt{count}(w_{k-1} w_k )}{\\texttt{count}(w_{k-1} )} $\n",
    "\n",
    "Возникает проблема нулевых вероятностей!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Аддитивное сглаживание Лапласа\n",
    "\n",
    "$ P(w_k | w_{k-1}) = \\frac{\\texttt{count}(w_{k-1} w_k ) + \\alpha}{\\texttt{count}(w_{k-1} ) + \\alpha |V|} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![AiB](https://raw.githubusercontent.com/artemovae/nlp-course-sberbank/e70ab4acb696c00e170ea91d1bb28fd9ba31c170/img/aib.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "BOS А и Б сидели на трубе EOS\n",
    "\n",
    "BOS А упало Б пропало EOS\n",
    "\n",
    "BOS что осталось на трубе EOS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$P($ и $| $ A $) = \\frac{1}{2}$\n",
    "\n",
    "$P($ Б $| $ и $) = \\frac{1}{1}$\n",
    "\n",
    "$P($ трубе $| $ на $) = \\frac{2}{2}$\n",
    "\n",
    "$P($ сидели $| $ Б $) = \\frac{1}{2}$\n",
    "\n",
    "$P($ на $| $ сидели $) = \\frac{1}{2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Модели биграм в NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, TimeDistributed, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "names = [name.strip().lower() for name in open('dinos.txt').readlines()]\n",
    "print(names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "chars = [char  for name in names for char in name]\n",
    "freq = nltk.FreqDist(chars)\n",
    "\n",
    "print(list(freq.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cfreq = nltk.ConditionalFreqDist(nltk.bigrams(chars))\n",
    "cfreq['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cprob = nltk.ConditionalProbDist(cfreq, nltk.MLEProbDist)\n",
    "print('p(a a) = %1.4f' %cprob['a'].prob('a'))\n",
    "print('p(a b) = %1.4f' %cprob['a'].prob('b'))\n",
    "print('p(a u) = %1.4f' %cprob['a'].prob('u'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "log(cprob['a'].prob('a')) + log(cprob['a'].prob('b')) + log(cprob['a'].prob('c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "l = sum([freq[char] for char in freq])\n",
    "def unigram_prob(char):\n",
    "    return freq[char] / l\n",
    "print('p(a) = %1.4f' %unigram_prob('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[bi for bi in nltk.bigrams('aachenosaurus')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Задание 1\n",
    "\n",
    "1. Напишите функцию для оценки вероятности имени динозавра. \n",
    "2. Найдите наиболее вероятное имя динозавра из данного списка. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cprob[\"a\"].generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Задание 2\n",
    "\n",
    "Напишите функцию для генерации нового имени динозавра фиксированной длины. (если еще не сделали это на прошлом занятии :) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Нейронные языковые модели\n",
    "\n",
    "* Вход: $n$-грамы $w_{1:k}$\n",
    "* $v(w_i)$ – эмбеддинг слова $w_i$, $v(w_i) \\in \\mathbb{R}^{d_{emb}}$, $d_{emb}$ – размерность эмбеддинга, $v(w) = E_{[w]}$\n",
    "* $x = [v(w_1), v(w_2), \\ldots , v(w_k)]$\n",
    "\n",
    "$\\widehat{y} = P(w_i | w_{1:k} ) = \\texttt{LM}(w_{1:k}) = \\texttt{softmax}(hW^2 +b^2)$\n",
    "\n",
    "$h = g(xW^1+b^1)$\n",
    "\n",
    "$w_i \\in V$, $E \\in \\mathbb{R}^{|V|\\times d_{emb}}, W^1 \\in \\mathbb{R}^{k \\cdot d_{emb} \\times d_{hid}}, b^1 \\in \\mathbb{R} ^ {d_{hid}}, W^2 \\in \\mathbb{R}^{d_{hid} \\times |V|}, b^2 \\in \\mathbb{R} ^ {|V|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![nnlm](https://raw.githubusercontent.com/artemovae/nlp-course-sberbank/e70ab4acb696c00e170ea91d1bb28fd9ba31c170/img/nlm1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Семплирование в нейронных языковых моделях \n",
    "### (Генерация текстов с помощью нейронных языковых моделей)\n",
    "\n",
    "1. Задать начальную последовательность символов длины $k$ (/слов)\n",
    "2. Предсказать распределение вероятностей слов с условием на $k$ предыдущих слов\n",
    "3. 1. Выбрать слово с наибольшей вероятностью\n",
    "3. 2. Выбрать слово по предсказаному распределению\n",
    "4. Сдвинуть окно на одно слово и повторить \n",
    "\n",
    "#### Линейный поиск  (beam search)\n",
    "Всегда помним $h$ наиболее вероятных гипотез:\n",
    "1. Для генерации первого слова в последоватительности генерируем $h$ кандидатов, а не 1\n",
    "2. Генерируем $h \\times h$ кандидатов для второго слова и храним только $h$ наиболее вероятных\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![nnlm](https://raw.githubusercontent.com/artemovae/nlp-course-sberbank/e70ab4acb696c00e170ea91d1bb28fd9ba31c170/img/nlm2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "alphabet = list(set(chars))\n",
    "print('total chars:', len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "maxlen = 5\n",
    "step = 1\n",
    "ngrams = []\n",
    "next_chars = []\n",
    "for name in names:\n",
    "    for i in range(0, len(name) - maxlen, step):\n",
    "        ngrams.append(' '.join([char for char in name[i: i + maxlen]]))\n",
    "        next_chars.append(name[i + maxlen])\n",
    "print('nb ngrams:', len(ngrams))\n",
    "print(ngrams[0],next_chars[0])\n",
    "print(ngrams[1],next_chars[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=len(alphabet))\n",
    "tokenizer.fit_on_texts(ngrams)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(ngrams)\n",
    "X_train = pad_sequences(sequences, maxlen=maxlen)\n",
    "sequences = tokenizer.texts_to_sequences(next_chars)\n",
    "y_train = tokenizer.sequences_to_matrix(sequences)\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "char_index = tokenizer.word_index\n",
    "index_char = {i: c for c, i in char_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(len(alphabet), 50, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation = 'softmax'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation = 'softmax'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(alphabet), activation = 'softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for iteration in range(1, 100):\n",
    "    X_train_shuffled, y_train_shuffled = shuffle(X_train,y_train)\n",
    "    model.fit(X_train_shuffled, y_train_shuffled, batch_size=len(X_train), epochs=1, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) #/ temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.choice(range(len(alphabet)), p = preds)\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "generated = ''\n",
    "seed = 'katya'\n",
    "generated += seed\n",
    "print('----- Generating with seed: \"' + seed + '\"')\n",
    "print(generated)\n",
    "\n",
    "for i in range(8):\n",
    "    sequences = tokenizer.texts_to_sequences([' '.join([char for char in generated[-maxlen:]])])\n",
    "    X_pred = pad_sequences(sequences, maxlen=maxlen)\n",
    "    preds = model.predict(X_pred, verbose=0)[0]\n",
    "    next_index = sample(preds)\n",
    "    next_char = index_char[next_index]\n",
    "    generated += next_char\n",
    "    print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Задание 3\n",
    "\n",
    "Измените код выше так, чтобы генерировались панграмы – имена динозавров, не содержащие повторяющихся букв"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Задание 4\n",
    "\n",
    "Измените функцию семлирования `sample`: добавьте параметр `t`, изпольузуемый для шкалирования вероятностей  `preds`: ```\n",
    "preds /= t\n",
    "``` \n",
    "\n",
    "Как использование этого параметра влияет на генерируемые имена?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import  ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(dist) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 100)) \n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=top_words);\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() \n",
    "\n",
    "plt.savefig('w2v_cluster.png', dpi=200) #save figure as ward_clusters"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
