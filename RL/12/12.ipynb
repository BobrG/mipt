{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Имитационное обучение\n",
    "## Алгоритм SQIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теория\n",
    "\n",
    "### Soft-Q learning\n",
    "\n",
    "Soft-Q Learning - это ряд методов, которые используют аугментированный энтропией возврат (return). \n",
    "\n",
    "Подбронее:\n",
    "\n",
    "https://arxiv.org/pdf/1702.08165\n",
    "\n",
    "https://arxiv.org/pdf/1704.06440\n",
    "\n",
    "### SQIL\n",
    "Подробнее https://arxiv.org/pdf/1905.11108.pdf\n",
    "\n",
    "Псевдокод алгоритма:\n",
    "<img src=\"sqil.png\">\n",
    "\n",
    "Краткое описание из статьи:\n",
    "<img src=\"paper.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "# импортируем необходимые библиотеки\n",
    "\n",
    "import gym\n",
    "import collections\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# если видеокарта доступна, то будем ее использовать\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать среду CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1. Подготовьте экспертные данные, используя любой алгоритм, который мы изучали на семинарах. \n",
    "Сгенерируйте необходимое, на ваш взгляд, количество экспертных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expert_data = \n",
    "#~~~~~~~~ Ваш код здесь ~~~~~~~~~~~            \n",
    "raise NotImplementedError            \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Воспользуемся реплей буфером с одного из прошлых семинаров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "\n",
    "    def __init__(self, max_size):\n",
    "        \"\"\"\n",
    "        создаем структуру для хранения данных\n",
    "        \"\"\"\n",
    "        self.buffer = collections.deque(maxlen=max_size)\n",
    "\n",
    "    def put(self, transition):\n",
    "        \"\"\"\n",
    "        помещаем данные в replay buffer\n",
    "        :param transition: (s, a, r, next_s, done_mask)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.buffer.append(transition)\n",
    "        \n",
    "    def sample(self, n):\n",
    "        \"\"\"\n",
    "        сэмплируем батч заданного размера\n",
    "        :param n: размер мини-батча\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "\n",
    "        # сэмплируем случайный батч и заполняем s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst\n",
    "        \n",
    "         \n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "        \n",
    "\n",
    "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "               torch.tensor(done_mask_lst)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        возвращает размер replay buffer'а\n",
    "        :return: len(replay_buffer)\n",
    "        \"\"\"\n",
    "        \n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2. Напишите общий реплей буфер. Добавьте возможность задавать соотношение сэмплов взятых из каждого буфера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergedReplayBuffer():\n",
    "    \n",
    "    def __init__():\n",
    "        self.expert_buffer = ReplayBuffer()\n",
    "        self.agent_buffer = ReplayBuffer()\n",
    "    \n",
    "    def put_agent(self, transition):\n",
    "        # помещаем данные агента в реплей буфер\n",
    "        #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~            \n",
    "        raise NotImplementedError            \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    def put_expert(self, transition)\n",
    "        # помещаем экспертные данные в реплей буфер\n",
    "        #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~            \n",
    "        raise NotImplementedError            \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    def sample(self, n, rate=0.5):\n",
    "        # сэмплируем батч заданного размера и заданного соотношения\n",
    "        #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~            \n",
    "        raise NotImplementedError            \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        возвращает размер replay buffer'а\n",
    "        :return: len(replay_buffer)\n",
    "        \"\"\"\n",
    "        \n",
    "        return len(self.expert_buffer) + len(self.agent_buffer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.  Реализуйте Off-Policy алгоритм, который будет использовать для обучения общий буфер.\n",
    "Можно воспользоваться обычной версией DQN с семинара 7 https://github.com/Tviskaron/mipt/blob/master/RL/07/07_solution.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4. Постройте графики сходимости для вашего алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бонус: Реализуйте SQIL по статье, использя Soft-Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
