{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CpceefzmsN45"
   },
   "source": [
    "## Динамическое программирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uX-Ky982sN47"
   },
   "source": [
    "Рассмотрим алгоритм итерации по оценкам состояния $V$ (Value Iteration):\n",
    "    $$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n",
    "На основе оценки $V_i$ можно посчитать функцию оценки $Q_i$ действия $a$ в состоянии $s$:\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n",
    "$$V_{(i+1)}(s) = \\max_a Q_i(s,a)$$\n",
    "\n",
    "Зададим напрямую модель MDP с картинки:\n",
    "<img src=\"mdp.png\" caption=\"Марковский процесс принятия решений\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ebF4HaPsN48"
   },
   "outputs": [],
   "source": [
    "\n",
    "transition_probs = {\n",
    "  's0':{\n",
    "    'a0': {'s0': 0.5, 's2': 0.5},\n",
    "    'a1': {'s2': 1}\n",
    "  },\n",
    "  's1':{\n",
    "    'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "    'a1': {'s1': 0.95, 's2': 0.05}\n",
    "  },\n",
    "  's2':{\n",
    "    'a0': {'s0': 0.4, 's1': 0.6},\n",
    "    'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "  }\n",
    "}\n",
    "rewards = {\n",
    "  's1': {'a0': {'s0': +5}},\n",
    "  's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "import numpy as np\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "0bcgBOZbsN5A",
    "outputId": "9ff86175-e1cd-4d8b-d5ec-72f1cfb23cd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_states = ('s0', 's1', 's2')\n",
      "possible_actions('s1') =  ('a0', 'a1')\n",
      "next_states('s1', 'a0') =  {'s0': 0.7, 's1': 0.1, 's2': 0.2}\n",
      "reward('s1', 'a0', 's0') =  5\n",
      "transition_prob('s1', 'a0', 's0') =  0.7\n"
     ]
    }
   ],
   "source": [
    "print(\"all_states =\", mdp.get_all_states())\n",
    "print(\"possible_actions('s1') = \", \n",
    "      mdp.get_possible_actions('s1'))\n",
    "print(\"next_states('s1', 'a0') = \", \n",
    "      mdp.get_next_states('s1', 'a0'))\n",
    "print(\"reward('s1', 'a0', 's0') = \", \n",
    "      mdp.get_reward('s1', 'a0', 's0'))\n",
    "print(\"transition_prob('s1', 'a0', 's0') = \", \n",
    "      mdp.get_transition_prob('s1', 'a0', 's0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WYDjsDv2sN5F"
   },
   "source": [
    "### Задание 1\n",
    "\n",
    "Реализуем итерационное вычисление функций $V$ и $Q$ и применим их для заданного вручную MDP. Вначале вычисляем оценку состояния-действия:\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zVSz8FCRsN5G"
   },
   "outputs": [],
   "source": [
    "def get_action_value(mdp, state_values, state, action, \n",
    "                     gamma):\n",
    "    \"\"\" Вычислеям Q(s,a) по формуле выше \"\"\"\n",
    "    # вычислеяем оценку состояния\n",
    "    # Q = \n",
    "    #~~~~~~~~~~ Решение ~~~~~~~~~~~~~~~\n",
    "    \n",
    "     \n",
    "    Q = sum([(prob * (mdp.get_reward(state, action, s) + gamma * state_values[s])) \n",
    "             for s, prob in mdp.get_next_states(state, action).items()])\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U8rRYvrpsN5I"
   },
   "outputs": [],
   "source": [
    "test_Vs = {s : i for i, s in \n",
    "    enumerate(sorted(mdp.get_all_states()))}\n",
    "assert np.allclose(\n",
    "    get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1tEz93sasN5K"
   },
   "source": [
    "Теперь оцениваем полезность самого состояния:\n",
    "\n",
    "$$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QA8c-2_fsN5K"
   },
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\" Считаем следующее V(s) по формуле выше.\"\"\"\n",
    "    if mdp.is_terminal(state): \n",
    "        return 0\n",
    "    # V = \n",
    "    #~~~~~~~~~~ Решение ~~~~~~~~~~~~~~~\n",
    "    \n",
    "     \n",
    "    V = max([sum([prob*(mdp.get_reward(state, action, s) + gamma * state_values[s]) \n",
    "                  for s, prob in mdp.get_next_states(state, action).items()])\n",
    "             for action in mdp.get_possible_actions(state)])\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ym4og12hsN5N"
   },
   "outputs": [],
   "source": [
    "test_Vs_copy = dict(test_Vs)\n",
    "assert np.allclose(\n",
    "    get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "emA2sv-QsN5P"
   },
   "source": [
    "Теперь создаем основной цикл итерационного оценки полезности состояний с критерием остановки, который проверяет насколько изменились оценки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "er9KwtjBsN5Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0 | diff: 3.50000 | V(start): 0.000 \n",
      "iter    1 | diff: 1.89000 | V(start): 0.000 \n",
      "iter    2 | diff: 1.70100 | V(start): 1.701 \n",
      "iter    3 | diff: 1.13542 | V(start): 1.854 \n",
      "iter    4 | diff: 0.73024 | V(start): 2.584 \n",
      "iter    5 | diff: 0.61135 | V(start): 3.186 \n",
      "iter    6 | diff: 0.54664 | V(start): 3.590 \n",
      "iter    7 | diff: 0.49198 | V(start): 4.082 \n",
      "iter    8 | diff: 0.42210 | V(start): 4.463 \n",
      "iter    9 | diff: 0.36513 | V(start): 4.816 \n",
      "iter   10 | diff: 0.32862 | V(start): 5.145 \n",
      "iter   11 | diff: 0.29262 | V(start): 5.429 \n",
      "iter   12 | diff: 0.26189 | V(start): 5.691 \n",
      "iter   13 | diff: 0.23503 | V(start): 5.925 \n",
      "iter   14 | diff: 0.21124 | V(start): 6.135 \n",
      "iter   15 | diff: 0.19012 | V(start): 6.325 \n",
      "iter   16 | diff: 0.17091 | V(start): 6.496 \n",
      "iter   17 | diff: 0.15366 | V(start): 6.649 \n",
      "iter   18 | diff: 0.13830 | V(start): 6.788 \n",
      "iter   19 | diff: 0.12445 | V(start): 6.912 \n",
      "iter   20 | diff: 0.11200 | V(start): 7.024 \n",
      "iter   21 | diff: 0.10079 | V(start): 7.125 \n",
      "iter   22 | diff: 0.09071 | V(start): 7.216 \n",
      "iter   23 | diff: 0.08164 | V(start): 7.297 \n",
      "iter   24 | diff: 0.07347 | V(start): 7.371 \n",
      "iter   25 | diff: 0.06612 | V(start): 7.437 \n",
      "iter   26 | diff: 0.05951 | V(start): 7.496 \n",
      "iter   27 | diff: 0.05356 | V(start): 7.550 \n",
      "iter   28 | diff: 0.04820 | V(start): 7.598 \n",
      "iter   29 | diff: 0.04338 | V(start): 7.641 \n",
      "iter   30 | diff: 0.03904 | V(start): 7.681 \n",
      "iter   31 | diff: 0.03514 | V(start): 7.716 \n",
      "iter   32 | diff: 0.03163 | V(start): 7.747 \n",
      "iter   33 | diff: 0.02846 | V(start): 7.776 \n",
      "iter   34 | diff: 0.02562 | V(start): 7.801 \n",
      "iter   35 | diff: 0.02306 | V(start): 7.824 \n",
      "iter   36 | diff: 0.02075 | V(start): 7.845 \n",
      "iter   37 | diff: 0.01867 | V(start): 7.864 \n",
      "iter   38 | diff: 0.01681 | V(start): 7.881 \n",
      "iter   39 | diff: 0.01513 | V(start): 7.896 \n",
      "iter   40 | diff: 0.01361 | V(start): 7.909 \n",
      "iter   41 | diff: 0.01225 | V(start): 7.922 \n",
      "iter   42 | diff: 0.01103 | V(start): 7.933 \n",
      "iter   43 | diff: 0.00992 | V(start): 7.943 \n",
      "iter   44 | diff: 0.00893 | V(start): 7.952 \n",
      "iter   45 | diff: 0.00804 | V(start): 7.960 \n",
      "iter   46 | diff: 0.00724 | V(start): 7.967 \n",
      "iter   47 | diff: 0.00651 | V(start): 7.973 \n",
      "iter   48 | diff: 0.00586 | V(start): 7.979 \n",
      "iter   49 | diff: 0.00527 | V(start): 7.984 \n",
      "iter   50 | diff: 0.00475 | V(start): 7.989 \n",
      "iter   51 | diff: 0.00427 | V(start): 7.993 \n",
      "iter   52 | diff: 0.00384 | V(start): 7.997 \n",
      "iter   53 | diff: 0.00346 | V(start): 8.001 \n",
      "iter   54 | diff: 0.00311 | V(start): 8.004 \n",
      "iter   55 | diff: 0.00280 | V(start): 8.007 \n",
      "iter   56 | diff: 0.00252 | V(start): 8.009 \n",
      "iter   57 | diff: 0.00227 | V(start): 8.011 \n",
      "iter   58 | diff: 0.00204 | V(start): 8.014 \n",
      "iter   59 | diff: 0.00184 | V(start): 8.015 \n",
      "iter   60 | diff: 0.00166 | V(start): 8.017 \n",
      "iter   61 | diff: 0.00149 | V(start): 8.019 \n",
      "iter   62 | diff: 0.00134 | V(start): 8.020 \n",
      "iter   63 | diff: 0.00121 | V(start): 8.021 \n",
      "iter   64 | diff: 0.00109 | V(start): 8.022 \n",
      "iter   65 | diff: 0.00098 | V(start): 8.023 \n",
      "Принято! Алгоритм сходится!\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(mdp, state_values=None,\n",
    "    gamma = 0.9, num_iter = 1000, min_difference = 1e-5):\n",
    "    \"\"\" выполняет num_iter шагов итерации по значениям\"\"\"\n",
    "    # инициализируем V(s)\n",
    "    state_values = state_values or \\\n",
    "    {s : 0 for s in mdp.get_all_states()}\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        # Вычисляем новые полезности состояний, \n",
    "        # используя функции, определенные выше. \n",
    "        # Должен получиться словарь {s: new_V(s)}\n",
    "        # new_state_values = \n",
    "        #~~~~~~~~~~ Решение ~~~~~~~~~~~~~~~\n",
    "        \n",
    "         \n",
    "        new_state_values = {s : get_new_state_value(mdp, state_values, s, gamma) for s in mdp.get_all_states()}\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "        assert isinstance(new_state_values, dict)\n",
    "\n",
    "        # Считаем разницу\n",
    "        # diff = \n",
    "        #~~~~~~~~~~ Решение ~~~~~~~~~~~~~~~\n",
    "        \n",
    "         \n",
    "        diff =  max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "        print(\"iter %4i | diff: %6.5f | V(start): %.3f \"%\n",
    "          (i, diff, new_state_values[mdp._initial_state]))\n",
    "        \n",
    "        state_values = new_state_values\n",
    "        if diff < min_difference:\n",
    "            print(\"Принято! Алгоритм сходится!\")\n",
    "            break\n",
    "            \n",
    "    return state_values\n",
    "\n",
    "state_values = value_iteration(mdp,\n",
    "        num_iter = 100, min_difference = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3cKoN2-tsN5V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final state values: {'s0': 8.023123818663871, 's1': 11.163174814980803, 's2': 8.915559364985523}\n"
     ]
    }
   ],
   "source": [
    "print(\"Final state values:\", state_values)\n",
    "assert abs(state_values['s0'] - 8.032)  < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bSIBT-49sN5X"
   },
   "source": [
    "По найденным полезностям и зная модель переходов легко найти оптимальную стратегию:\n",
    "$$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x9m7NArGsN5Y"
   },
   "outputs": [],
   "source": [
    "def get_optimal_action(mdp, state_values, state, \n",
    "                       gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state): return None\n",
    "    \n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    # выбираем лучшее действие\n",
    "    # i = \n",
    "    #~~~~~~~~~~ Решение ~~~~~~~~~~~~~~~\n",
    "    \n",
    "     \n",
    "    i = np.argmax([get_action_value(mdp, state_values, state, action, gamma) for action in actions])\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    return actions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tCt9IjLnsN5Z"
   },
   "outputs": [],
   "source": [
    "assert get_optimal_action(mdp, \n",
    "        state_values, 's0', gamma=0.9) == 'a1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-omPxnyksN5b"
   },
   "source": [
    "### Задание 2\n",
    "\n",
    "Теперь проверим работу итерации по ценностям на классической задаче FrozenLake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QhBAOCyEsN5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*FFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "iter    0 | diff: 1.00000 | V(start): 0.000 \n",
      "iter    1 | diff: 0.90000 | V(start): 0.000 \n",
      "iter    2 | diff: 0.81000 | V(start): 0.000 \n",
      "iter    3 | diff: 0.72900 | V(start): 0.000 \n",
      "iter    4 | diff: 0.65610 | V(start): 0.000 \n",
      "iter    5 | diff: 0.59049 | V(start): 0.590 \n",
      "iter    6 | diff: 0.00000 | V(start): 0.590 \n",
      "Принято! Алгоритм сходится!\n"
     ]
    }
   ],
   "source": [
    "from mdp import FrozenLakeEnv\n",
    "mdp = FrozenLakeEnv(slip_chance=0)\n",
    "\n",
    "mdp.render()\n",
    "state_values = value_iteration(mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-kS3QDqNsN5h"
   },
   "source": [
    "Визуализируем нашу стратегию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wRbL3izdsN5h"
   },
   "outputs": [],
   "source": [
    "def draw_policy(mdp, state_values, gamma=0.9):\n",
    "    \"\"\"функция визуализации стратегии\"\"\"\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    h, w = mdp.desc.shape\n",
    "    states = sorted(mdp.get_all_states())\n",
    "    V = np.array([state_values[s] for s in states])\n",
    "    Pi = {\n",
    "        s: get_optimal_action(mdp, state_values, s, gamma)\n",
    "        for s in states}\n",
    "    plt.imshow(V.reshape(w, h),\n",
    "               cmap='gray', interpolation='none',\n",
    "               clim=(0, 1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(h) - .5)\n",
    "    ax.set_yticks(np.arange(w) - .5)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    Y, X = np.mgrid[0:4, 0:4]\n",
    "    a2uv = {'left': (-1, 0), 'down': (0, -1),\n",
    "            'right': (1, 0), 'up': (-1, 0)}\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            plt.text(x, y, str(mdp.desc[y, x].item()),\n",
    "                     color='g', size=12,\n",
    "                     verticalalignment='center',\n",
    "                     horizontalalignment='center',\n",
    "                     fontweight='bold')\n",
    "            a = Pi[y, x]\n",
    "            if a is None: continue\n",
    "            u, v = a2uv[a]\n",
    "            plt.arrow(x, y, u * .3, -v * .3,\n",
    "                      color='m', head_width=0.1,\n",
    "                      head_length=0.1)\n",
    "    plt.grid(color='b', lw=2, ls='-')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQiqx1QesN5k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iteration 29\n",
      "iter    0 | diff: 0.00000 | V(start): 0.198 \n",
      "Принято! Алгоритм сходится!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAAC4CAYAAABQMybHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztnXl8VNXd/9+zZWay7xsEkhCWsBN2iAQExAVw37U/d0sXtfrz6aPPU2uhL221au3jY7WupbVirT/rghYtKIgIhD0BkpCwJJB93yaZzPL745oZhpkkc2fO6DC579fLl/fce+eTO9zvPXPuOedzviq73Y6CQqii/r4vQEEhkCgBrhDSKAGuENIoAa4Q0igBrhDSKAGuENIoAa4Q0igBrhDSKAGuENJohzpBpVLdA9wjlSJmwoQAX5KCwtAYDIcxmUyqoc5TyRmqV6lm2fPyxAzt79u3F4B58+YL0du58xsACgoWC9ED2Lr1SwBWrLhYiN6mTf8C4IorrhSiB/DPf74PwI033iRE7+23/wbAXXfdLUTv1VdfAeD++x8QotfP22+/TV1d3ZABrjRRFEIaJcAVQholwBVCmiFfMr2lN7yX05NO0xXfhVVrRWvWYuwwknEoA323XrbevoJ9mI1mt/1Tvp5CREeET9e4c95Oeo29bvtnFs4ksjNStt7WGVvpMfS47Z9/cD7R3dGy9TZN3IQpzOS2f3HpYmJNsbL1AD7M/pAuXZfb/otPXkxcb5xsvQ2pG+jUdrrtv7LuShL6Eny6xtejX6dD0+G2/6b2m0iyJvmk2Y+wAD8++zimGBNRDVHou/SYDWY6EzrpM/T5FOD9xNbHYug2OMo6s87va41vjMdoMgrTTGpOwtjr1AuzhPmll9KWQoTZ+RDrLb7/+/WT3plOpNn5EOut/mlmmDKItjgfYoPVMMjZ3pFlziLGFuMoG23GQc72DiEBbtFZMMWY0Jg15HyTgwrp5damtsGQ77mDk3w6mfj6eAFX6SStJo3ExkRheiPqR5DSkiJMb3TzaNLb0oXpAYxpG8PIzpHC9MZ3jSezJ1OYHsAk8yTG9I0RqikkwDUWDWqLGmuYlZKCEqIao4hsiiSqIQqNVeM4T2VXYVfJ62asH1lPe3y7o5xZkumXHkBNWg2tsa2Ock55jrTRLyXzoTyTfIbmmGZHOfdkrvOgXb7eqfhTNEY2OspTz0z1Sw+gIqaCOmOdozyzYaZfmqURpdToaxzl+W3+d/ceDjvMae1pR7nAVOC3ppAAV9lVjD4wmspplZhiTJhiTNSPqSfMFMas7bOIbY5lWss0FjYs5LO0z9iZuNNr7dbkVpfy3H1zATBYDNxecjtH447yWcZnsq63ObHZpbzgwAIAVlSsILY3ls+yP6M0odRrvYb4BpfyoqJFAKS1pbGsZBl7Ru1hV9Yur/XqYupcykuOLHFsX7njSpqimtg+eTsNsQ3nfnRAqiOrXcrLji0DILc8l+zKbAqnFVKWVea1XpWxyqUsIsBPhJ1wKQdNgAPEVccRUxtDqi6VRGMiG/M20hLZAqPg4a8fdpx35ekrqTHWsNtL3UmFk3hh4wsDHp/aNBWNXcOnMq71rk/u4ubdN3s8ZsfO9Yev54n8J7zWW7J3CY999NiAxxecWECnvpMPvdSbc3wOv13/2wGPG3uNXLT3It5a+pbX13jLrlu489M7BzxesLvA7UEdjJuKbuLazdey8ZqNXn9mKNZtWEfnwk56It1f3H1FSIDbVXa64rqIbI6kvree+t56Ik5G0DK5hUMJh3go7yEmtE1gWe0yPkn/hONRx73W7tP08ci8R9z2h1nDuOPoHRTHF7MzdSfs/qnXmt9kfMMx4zG3/Rcev5AEUwKbszZj0Vi81muJaOE3K37jtj+5PZmLD1/MzuydlCWXQYmXgip4/ornPR66fMflNMQ0sHfsXq+vD+DUyFP86cY/ue3POZHD+OPj2T19N81xzR4+6ZmY1hhiW33r2RmMsJ6w4Atwm9pGWX4Zhg4DxjYjaqua1lSpaRHdIL1pl8SUUBLj7R0eGrPGzEuTXxKmB7Ale4tQvfroetbPXy9U84MFHwjVK88qpzyrXKhmMCEkwNU2NckVyXQkdtCe3I5NYyOsJ4ykk0mklIvrXVBQkIuwl8yRh8V1QQHkbc0Tqgcwb+c8oXoF+/1/CTqbFUdWCNUDWH18tVC9G2pvAGDGmRnCNO9ovwOA/JJ8tswU+yuqDNUrhDRKgCvIxthlZGzJWACm7J0iRHP++1I348L/t9A5HiEAJcAVZNNr6KXH0INVbaUpqUmIZmtyK3aVndakVr9Hv89GCXAF2dg0NvbO20tdWh3VGdVDf8ALymeW05bYxuH8w0L0+pH9kqlWi30mgl0PQKsVNh4GgMHg/8SkcwkPDxeqFxk5+OzKxumNNE5vJBLvZmFGRw8xuzIa9v1w37ebQ8/E1Gg0Q54DPljWYI/X5ysoBIq0tHSqq6v9t6ypVKp7VCrVHpVKtQe8H8pVUAgGZNfgs2aJeQPYs6cQgAULFgrR27HjawCWLLlQiB7AF19IfbKXXbZSiN7GjR8DcMMNNwrRA9iw4W0A7rzzLiF6r732KgAPPPAzIXq///1zAPziFwPP1fGFV199VUwNrqBwPqMEuEJIowS4QkgjrP/r0IWHMIe7m4QnbptIeLv8Lqy9F+z1aBCe9s00n03H38z9xqNJeNaeWUR1RcnW2zJ1Cya9u0k4vzifGFOMh08MzodjPqRb1+22f8WJFT4ZhAHeSXvHo0n4itorfDIJvxb9Gh1qDwbhjptItib7dI1/UP+BNlWb2/67rXeTSqpPmv2I7eAFYupi0Hc5Da1as39/Iq4hzsV07K8eQEJTgovpOKzPP5Nwcmsy4T3Oh9hfk7BogzB4MAnb/OuLz+pzNQiH2/zvhx9rH0uc3fkgh+O/pvAAT6xMJK7Ot9rGE8mnk0lo8G05goFIq0kjqcm/5QjOJqMhg9RW/2qas8luzRZqEAYY1zWOTFOmML1J5knk9OUI0wOYbpvOBMFrXwoP8MZRjXQkOH/CRh0ZBUB6dzrTmqexLWWbx3U6BuJc03FWaRYAGpuGZZXLOBp/lMroSlnXeK7peGyFNHFoUu0kwi3h7Evbh1Vj9VqvKqmKpijnnIxJVZMAiDJFMfPETPZn7qct3P0neCCOxx6nPrzeUc6rd04dzivOoymuiVPpp2TN2SiLKKNWX+soz2uVpg4n1SSRVp3G0clH6dP3ea13rkF4sWmx9xczAAfUBzhlP+Uor7D7P31YeIC3pbjeyHfWv+NSXlS3iBcmvOD1eGhLUotL+W9v/M2lvLB6IbtTd7NDxjU2JbhOEPrLq39xbNuxU3CygOcWPOe1Xn1svUv5jZfecCnnnchj44yNeOtePNcg/PIfXnYpW1VWapJq2Hih937Ic03CLzzj6nOdWDSRj6/62Gu9EzpXg7CIAD+mOuby0K6wBmGAT9o5iZv23oTaLnXQ9DvoJ7dOJtISyeHYw7SGtQ4m4cL4/eNZdXAVSSapSbE7RbIrGywGpjZNpU3fxtH4o1A7mIoriwoXsXq/0wiwN13yN86snolVZWV/6n5sKpvXenNK53DNrmscC/4cHHUQgNTWVFLaUzgTf4b66PrBJFzIP51PweEC0uultVGOjDniODaxYiJmnVmWAx7gsqrLWP31alR2KYKOjZc8qVnlWWitWk5lnaLX4P5SPxA/3fFTrvrsKj54QJyFbt2GdegW6ehK8f4XfiiEB3iPpof3R7/vtv99+/tEm6Np1Xsf3P3sSPdcP2/q2USbvk322ijNxmY2jnev/f495t+o7WpMOveekcGwqq1szd3qfsAOMaYYWc2TfipGV1AxusJt/95Je+nR92DTeP8AAvQYe9i90H0tg8L5heh79Jgi5H3n8wXhAT4QNpXNp+AejFaDWL1erfc1mFeo8Cm4B6M73L0b0R9sGlvIBjcoAz0KIY6wGnzqlqlDnySDmV/NHPokmczfJSabRD8XHhI3sQtgdYVYgzDA9TXXC9W7s11aPGha9TRhmvfZ7gMk0/GuRd6vAOYNSg2uIBu1RU3GkQwAEqrEjFGk7U8DYOQesf3/SoAryCasJwyNTXLUJJwRE+ApxdL6OYmliYrpWOH7pSeyh1MTT2HRWjg+w/tl+AajYqnUY1RxYYVQ0/F31ouiEFoUFxRTOrcUi977NRwHoyO9g50/3IkpQWyPjuLJVDgvGTkyg6qqKsWTqTC8kV2DT5/u/SSkwThwYD8As2bNFqIn2uMJTp+nuEah9G/9gx/8H0F6sH79nwFYs+ZHQvT++McXAXjssV8K0Vu79lcAPPPMs0L0+nnuuefE1OAKCuczSoArhDRKgCuENMK6CQ9fdJi+cPcJ8+O+GEd4m3zrkWiPJwj2eT4AxAIbcKYmyQRuA3oA94wmQ/JexnsezSArT68k3uxbKsW/Jv7VY5LVa5uuJdEiP5Xi86rnPfon77Hd47N/8tedv6bF3uK2/8HwBxmhGeGTZj/C+8Gja6MJ63J6HLW9/v0J0R5PCIzPUyQju0YSZXGaoEUkWR3dO1qoJ3OsfSzxOB86Ef7JiZqJJKidI6ORKvnZp89F+J2NPxVPbI245ESiPZ4QGJ+nSHI6chjVPUqoZq4pl6zeLGF6M+wzhPsn5+jmMEUnZr3xfoQHePPoZjoTncsUjCySJs+o7WrizHE06eWtJz2Qx9MfBvJ5+sQMpKYJ4MWiqF5RHlXukrR1dpOzKzWiM4IeY48szyjAUeNRzujOOMr5nfmANHHKYDLQHSVvnvl+1X5OIdY/ubtvNxVWp8njCsMVfmsKD/D21HaX8s8//jkAk9smY7AZKIou4t2Md73WO9fjKSLAz/V5+hXg4/28GA+cjjjtUn7wgwcd2znHc+gN66Uwr5Bj49xTIQ7EKf0pOGv1iX7NzLJM6XjOKfYt2ue13jGV698WEeBHrEfgrOc2KAP8gi8uYO3WtQMen9Axgag+7xfZuWjLRTyy7REenvXw0Cd7yboN6yhTl1ER624Jk42nl0w/WVyzmDXvrSG50fNCOlqLlqxTWbIC/JYjt3Dn3wdOBDvi5AiK5xZ7rffj4h9zzT+u4YvHvvD6M0OxbsM64q6IozdJnLNKeIBXhVfxs+nuK5OmmdKY2jqVr5K+olvr/c+hiBes71JXCCrYeKlnx/zkosm0xLdwJv2Mx+MD0ZjeyN/X/N1tf0JNAsnVyRybcgxLmPcTp3TdOll/31tUFoFTCfkOZxPWGGuoMdZ8V38uZCme4n0t6w1NaU00pYnJsxOMKAM9CiGNsBp80meTREkBTo9nTq245cH6fZ75Jfn8bfzfhjh7CH7vYd9J4HHfJa+uutr3Dw/ALY23CNW7334/2GFc7Thhmv8d+d8A5JXkcWTlkSHOlkfQ1+CxZqlPPdwiJslShFkasYzplb/6q4ITQ5v0DqPp8S4Z1FDo2qQ2fViLfwuhnktQB7jRYuSC+gsAWF0pxnF+dblUS1528jK0tuAawTxfMLQYSKiQBsoyt2UK0cx+IxuArL9kgbw1jQYlqAPcpDVRHlWODRuFiYVCNPek7MGOneL4YixqMXar4UZPbA+dSZ3YVXZqp8pYM28QGuc1YsdO88xmoVEZ9FXYBxkfMK1lGhVRAvqsgaPxR9mevt2xxqGCD6igdFUpsSdj6UoVs45g0+wmDLUG6i6sG/pkGSieTIXzkszMLE6cOKF4MhWGN997nsz8/AuE6G3f/hUAy5YtF6IH8O9/fw7A6tWXC9H78ENpqeHbb79DiB7AG2+8/u2WWN/os896vz76YDz4oDSq/eabfxai18/jjz8upgZXUDifUQJcIaRRAlwhpFECXCGkCdpEsIULCz0ahKfvnE5kp29eve2ztntMBDt3/1yfEsF+Pulzj4lgC44W+JQI9t0R73pM2rq6erVPSVsDYYxe17HOo0H4oYiHfDYIP1T1EE1W9xmNv0r7FaP1o33S7Oe8SAR7dtJWXZ//85ATmxOFaqa0pRDR63Tl9yej8pWM7gxX07GfBuFAMFHrahCOUPmWffpsphmnkax1mjyiNf57AIM+EWxqdapwg3B6bTrJzb6lnfbEqMZRpLWlCdMb2zmW0Sb/aq5AM1c3V7hBeFHkImZGiM3s8Z0lgs1tzWV59XI+Hfkpx6K8t1rVptfSFuf0ZWaXSZNy9BY9txXfxpGEI+xM3ynrGqtTq2mJcf7Mjj8hGSuXHFtCQncCX+R8QVOE9yaAysRKl0Swk09PBiCpLYmL9l9E4dhCjqV7/52PRR6j1uCc4zG3Za7Xn/VIAIzRu/p2UW4td5SvNFwJgLZdy5hXxtA0u4mm+fKMFNs6t1HSU+Io35xws9/X+Z0ngr2n7B5eGveSrESwLTiD8a3X33I5nt6ZzojOEchxBjbGN7qU17+y3rFtx864hnE8teQpr/XqYlznT7z24msu5cv2XsYXvV/gbUbJqnDXpK1+B3gAjNFHLK7zttc+6urDNZ4xYqwzIoeDpoMu5aAM8LG7x5Jdme0o/2ay9CYzpWUKBXUFbErfxMnIk17r5R7MJfN0pmNq67MzpVVKDVYDtxXfRkl8CVtGbYGmH3utmVecR3a18xr/d8H/ArC8bDlxpjg2j90sa6bh7PLZTKhxrhHy+lJpdDGlNYWlB5eyb8w+jmQcAS/dZpeeupSH33iY9bevH/pkbwiAMfp24+3kdeahskqDiYcfPQxI87qzX8+mJa+F2uW18I33mus2rCP75mz6MrxPKT4UwgPcprLRZHD/afoy7Uu+TP3SpxHlDr370mMAT8590ic9i9pCa7h7js13p327nIVcTRW0RbgvZ9YW0UZZepnQlBzBRF+ceyCaE80U/booaL7zdztdVvSXDna9QGkGO0H0nZWBHoWQJmgTwc7+Wkzmh7PJ35MvVG/5YXEzFwGuPXMtAJHt/i86CQTEGP2LqF/4/uEBeCbjGQBGl4ymmmqh2koNHmzY4YJt0hTiBV8t+J4v5rsj/g1ppdrEVxKHjydzWKIC7FJ3pcoeRI3ZQGOTvrMdu9A2vBLgQcje2XuxaC0cmHHg+76U74y2K9qwG+y03tCqJIINdepT63n71re/78v4TrEmWKl6qWroE2WimI4VzktycsZy7NgxxXSsMLyRXYNPmyZmsZyDB6X25XBMBPvQQ/9XkB4888zvAFi7dp0Qvccek7oBX3rpZSF6P/zhvQB89NHHQvT6+dnPfiamBldQOJ9RAlwhpFECXCGkEdZNeOSiI/RFeEgEu2UcxjZ584JheCaCfSXiFdrV7W77b+26lWSbbw6kZ23P0or7zMk1qjWkqeS7kB6tf5RmW7Pb/v9K+C8ydBk+XeOdRXdSb6532/987vNkh2d7+IT3iE8EW6MkgvWXbEs2sTZnrlGjXX4FcS7jGOeSuDUC/zyUU/RTSNIkOcqRav/nz8yOmU2a3vnQxWj9X8M9IIlgY2rELS4/HBPBTu6bzFjLWKGaM1UzyVXlCtNbaFzIdMN0YXoAyxOXMz92vlDNgCeCHVEkLSUQY44hpzOH/XH7sanOmU1jh9y2XFp1rdREuCaq8jURbFxPHBntGRxKOuTWyxfsiWCLdcWc1jhzZS7pXeLYTi1LpTOhk84E9+UlBmOvfS8n7Ccc5UvVlwJgaDIQWRNJ48RGWW9kX5u+psxc5ihfF32dtGGB6N3RdE3uwhotL1nt542fU9zhtD3dnXG3rM97Qnwi2DTXNuS7f3VN+np5zeW8MOYFR1lv1XPfkftI7nVtY96QfwN14XU+JYJdfmo5C6sXorPruL78eqfmnBuoM9YFfSLY49rjLuVnnnjGpWzVWDmTe4ZDKw55rVlGmUv5yceedCn3xPRw8C5XT+RgFPUWuZR/8R+u02htOhv117q3qwejsM01yUFQBvj0bdN55OtH0FuldnObVgrQGIvUbOnSdGFROQeL7NhpD2t3BHi7rt2xH+A/3/1PKs2VlMW43qDB6NJ1YVfZwQ7d2m6sKquL5g8+/QEV6grpHH8JgN9xdfdqVm5dSUax9NLWE+FcrMjQJb079EbIS5Z6e/PtXLf+OtQWqZo2R0ov8GGd0vtSX0Qfdo33/x5rItdw6fpLMZyUrscSLd1TbbsUUnatHWuEvBp83YZ15P04D1umuPmywgO8NayVJyY84bbfaDUywjSC8ohylyaDWWPm5fEvM7JrJJ3aTlr1rQ4dgHcz35XdBt+RvoN9yftI60rjRIzzZ7lNLz1su1J3BXUbHBWU5pdSml/qdijuTBzdsd2yA7wnoYfCB93TwOg6dRibjLSPdu+9GQy71s6ZH3lIRmuD8KPhmMaasIfZ4RVZssL5zroPTBoT5ZHlAx4/Nz+7v/Roe1yCO1RoGeG+bJo/9EX20RcpzsWOGroneZ/JOtAoAz0KIY2wGnziZxNFSQHiPZ7gTAQrhAD4He/u8v+l6lweVD8oVO+JZPfmp7+8NkVaKCny8Ui6EVv7KzW4QlCgKZISymr2iUks248S4ApBQdibUm9O2DthiulYIfQw3yx1W5qvMw+vRLAKwwPrDCumR01YZ8jrOx8KxZOpcF6SmzuRI0eOKJ5MheGN7Bp8wgQxuclLSo72qwrR6/c7il35UdK8994fClF7+eWXAHjkkUeF6AE8+aTUbff8838Qonf//fd9uyX2vuzZs1eQnsStt94qpgZXUDifUQJcIaRRAlwhpBHWTVi+qhxLhPuaKZn/ysTQKjMNXgD8joHQfCvpLY95La9uuJpES6JsvRfDXqRN5Z4p4g7zHaTYU+RfIPCrtl959FA+HPUwI7Uj5YkF4r4AqzavosZU47b/rQveYnyMfxPuhfeDR5yJcMwxBtD0ih16DUZG9Ywi2uq08xht/nkoc6w5xNqdnsxwu28m67OZpJtEotr50InwUIrmguQLGBnhfOjiwvy3KgoP8NjjsUSdkZ81+HxmQvcEsnr9cAWdwzTbNMbZxgnTA5gXNo+pYeInsInk8lGXszh1sVBNnwNca9N6zETWmt1Kd7JzRljKft9+WoGA+B0DoVkSXkK13pmZYGG7c/k4tUWNTStvcsVB9UFOqU45ysutzkwSaosam8Ymuxdvp3kn5RbnfPyrwq+SJ3A2gbgvwAeVH7C3ydmd+NCkh/zWlB3gcX1xXN5wOXPb5/JR4kfUhEltpypbFRYsdI3oogtnX7lfAR4Av2MgNCsNlS7l6w9IPtCE+gRyD+ZSMaGCotlFnj7qkXKNqzHk5sPOfJFTP5hKZ2InpctKaRntvfnhcN9hl7JfAR6I+wJ8Vf+VS/l7CfBZ7bOY0z4HgFWNqxz7t1i30EUX6zasY3PvZmr1tQNJeE8A/I6B0Lz6xNX85M8/GfD4mJIxNCV5n/X3avPVrHlqDRqL5/eXqIYoxm8ez847vM/w/MsPf8nYzLE0FcjLPuyRQNwXJE/miqdXoE4T17knO8A/j/+c0/rTLGlZwoaUDTSHSW/oTTrpH+6lES8NuzZ4R0wHG+7d4LY/qiWKGTtnUDKthPr0evjKw4c9oYLND2/2eGjyh5PpSOmgKk/eYvHabi3hleE0ISDAA4i9S4AR/Czkt8FVcDTyKEcjjw597jCnI66DbZdsE6pZvNrLdMkKgDLQoxDiCOsmzPkoR5RUQPyOgdC8ueHmoU+SwY/MPxKqB/DLmF8CMLVkKi2z/HTkB+K+AB8t/QiAjsc9p2z3B6UGHw5820upsgRvWkJ737dtb/cFhf1CCfBhQPK/pFXDYg/EEtYYNsTZ3w/da7od/7fbxL1oKgE+DOga14VNY6Mvqg9znOAqUhDa+VpQg2a2BpVa3C+N4skcBnTldNGa10r75HYI0qlBYTeEYd1rxfATmRPzhkAJ8GHC6VvELo0nGlWEivAX/Z9U5qarmI4VzkemT5/B/v37FdOxwvBGdg0+fry8zAIDUVraP5lBrLn1kksuFaQHn376CQDXXXf9EGd6x9///s63W+KN0f/4x3tC1K655upvt8Tel7Y2ecszD0VBQYGYGlxB4XxGCXCFkEYJcIWQRlg3YcWqCiyR7g6f0Z+ODg7TMfDl9C8x6U1u+xcWLSS6W7415eOcj+kOc1/PennFcuJ6ffATBuB7ryldQ0Ofe+fA02OeJsso02YXoPsy5bUpVLZXuu3/6uavmJrsn81uWJqOk1qSCO9x9rmG9fk3fJ3WkUak2Wni7U/AFUzMjJpJaliqoxytFeg1E8TFWReTFet86BLD5a9McC7CAzymIiboDQ8ZDRmktPhhpTuH7NZsRnSMEKYXCJbGLWVO9Jzv+zIG5dbJt7IyZ6VQTeEB3jamDVOKsxmQvM+3HOtAwMytVUlVNEU5nS0TK6X0K9NPTSeyN5LCrEJ6da5ZzGLbY8k7msfBcQdpinN1xRyPPU59uDMn5Iy6Gf5dYAC+9+aWzRzucvoyb0+73XexAN2XvxT/he2ntzvKv1nsY5vnLIQH+LmmY78CPEDm1oY41zbpmy+/6di2Y2fWyVn8z9L/cewbWTuSldtWokLF+FPOi9r6wFa6w7qpiXJdtMbvAA/A997b4br4pV8BHqD78q8T/3IpB2WAr9uwjvST6Tw1+in/xQJkbl10cBEFRwsc5S0TtgCwoHwBADvG7MCqdi7E3hTbRElmCbknc6lKqaIyVXoh6q/lb9x1IyqjCovW/SXbJwLwvddtWMfs5Nl03idgoC6ApuO7X78b4yT/Fk46m4BMtlIJHakTT6exkz1Z7nNq9o/ajwoVFo1roJoMJr6c8yU7pu/ArDM7BvnMOmnqaWV6ZdC3wYcrymzCs7BqBk+fYQ4LzrnUCgOjDPQohDTCavAxH40BIL80nxMGP1NoB8jcuvjAYv8EzmFludgurUB87z+O/yMACSUJ9CbLy2/vRoDuS9Gd0qpf+x7f55+QB4TW4Nnd2QBk9WQRZQnuvvDhhG6vDgD9Nj24D+QGBbW/l1ZCq/lNDXJmuA6F0AA/e03scJt4d4aCb2hqpNFku8qOyhScHQCdO6Tene793c50SwIQGuCFUYW0aFvYH7mfurA6kdIKftCzvAdbhI3eZb3Y48UujSaKEY+PABWMWDsieE3HdpWdtZlrsagE9QcriEGAGfszAAAFN0lEQVQPrS+0YtcHZ3ADGCcamVI6BW2S2I49xZOpcJ6iwm63K55MheGN7Bp86tQ+IX/40KGDAMyaNVuI3p49hQDk518gRA9g+/b+9Y7F+hM/+uhjQXqwapXUVVlUJGbV2SlTJgPiPJQxMf2zscS/3AqpwRUUzmeUAFcIaZQAVwhphPXJHF1xlL4I9/b52M1jMbbJn/546MJDmMPdJzdN3DaR8HbfBpEKFxbSa3Qfrp6+czqRnTLzRgbAn3hn0Z3Um+vd9j+f+zzZ4dnyBYEVn66gurvabf+7S99lQuwE2XoB8U+OAPKBDMAIdAP1SB12fiYSET6bMKomCn2X05Oo7fXvT8TUxbjqmf2/5LiGOIwm50On69P5rSmS2TGzSdOnOcox2hi/NQtSC8iIzHCU4/T+JVkV5p+cCFyD1JZoAMoAPVLQTyH4Ajz+ZDwxNf7fkH4SKxOJq/M/4+3ZpFanktCQIFRTJMsTlzM/dr5QzSszr2TpiKXC9IT4J3XASqTgLgLex7FYPyrAf8+x+ABvzmymK8lpWUs/lA6AzqYjpSeF0+HyVjltHNVIR4IztcWoI6Mc2yO7RlJrrPWYkFZn1ZFoSqQm0j0Hem16LW1xzpzw2WW+/fwDAfEnft74OcUdzm6/uzPudmyrT6qxJdpAZovq/ZPvs6fROUj382k/B8DWYcNWbUM7Xl4oDOSftNvtdO/pJnx6OCrdEL14GUB/a3MrzuAGqUdVwLCL8ADvSOugA2dAPv3e0wBkmjIBOBF+gjdHvem1XltKm0u5X89gNZDck0y3ppv3Rr/nMr6aV5vHJScuwWA1UBNR45g6UDK7hF5jLy1JLbTgzFfjV4AHwJ9Y2FboUr7v9/c5tjVlGux6O+YbzfRd7f2YxNbarS7le397LwCWIunfRjdPR9RT3s8APdc/edva2wDoq+mjr7oPbZKWUS+OYtCnPuKs7dZv/78MqT3ez+NeX5JHhAf4vC/m8eTWJwc8ntqTisHm/UJA43eNZ+2WtST3ejYvh9nCSOtOc9mX2pWK1iZ9tbQu5zGNXZpV98A/H+BA+AGPNb9sAuBPfDTrUZa8sARt4QC3xwrqY/I6wH4X/jtm/sdMR9mC63e3lFiwm7wf9Fu/ZD1Z12ZhqZd0unFdAMnaaqW3bIj5511nbUcDzUAlcAjwb70fB8IDvMZQw/1T73fbn9CbQG5HLrvid9Gn9r7msagtPD3labf9apua+Q3zKY0ppdHQCLW/dhz7ZMwn7EndQ2ZbJoVphdhV0o1r1UvVxKasTUHdBkcNPY/1eDyk/VyLbbQN2zibx+MDoZ2qJbHIvVHbV9aH5ZAFw+WGoZsUZ6GJ1DD1mHsU2kw2Gl5vIHZVLPpRevivQUSqkHpMwoELgA+QXjLbCd4AH4gmfRPb9duHPtFLbGobX6d8PeDx+oh66iPcu9zOdyzLxc7U1I3ToRsnrhdJbVST8mMvF1XqAz4BrkJ6l0kDTgPi+igU07HC90wxUo29EOmlczpS06UcODzI57xEWIDnbsoVJQXA1C2CfqPOYvbXYiZ2AQHxJ7425TXfPzwAmy7ZJFSv3z8plMpv/wsAylC9QkijBLhCSKMEuEJIowS4QkijBLhCSCPTsqbqAEoF/v1EoDGI9QKhORyvMRDfebTdbk8a6iS53YSldrt9lo8X5IZKpdoTzHqB0ByO1xiI7+wtShNFIaRRAlwhpJEb4H8S/PeDXS8QmsPxGgPxnb1C1kumgsL5htJEUQhplABXCGmUAFcIaZQAVwhplABXCGn+P94mjRGy6KGCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mdp = FrozenLakeEnv(map_name='8x8',slip_chance=0.1)\n",
    "state_values = {s : 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(30):\n",
    "    clear_output(True)\n",
    "    print(\"after iteration %i\"%i)\n",
    "    state_values = value_iteration(mdp, \n",
    "                            state_values, num_iter=1)\n",
    "    draw_policy(mdp, state_values)\n",
    "    sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62xktmhWsN5n"
   },
   "source": [
    "Тестируем на различных вариантах окружения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZsHbVNzsN5o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0 | diff: 0.80000 | V(start): 0.000 \n",
      "iter    1 | diff: 0.57600 | V(start): 0.000 \n",
      "iter    2 | diff: 0.41472 | V(start): 0.000 \n",
      "iter    3 | diff: 0.29860 | V(start): 0.000 \n",
      "iter    4 | diff: 0.24186 | V(start): 0.000 \n",
      "iter    5 | diff: 0.19349 | V(start): 0.000 \n",
      "iter    6 | diff: 0.15325 | V(start): 0.000 \n",
      "iter    7 | diff: 0.12288 | V(start): 0.000 \n",
      "iter    8 | diff: 0.09930 | V(start): 0.000 \n",
      "iter    9 | diff: 0.08037 | V(start): 0.000 \n",
      "iter   10 | diff: 0.06426 | V(start): 0.000 \n",
      "iter   11 | diff: 0.05129 | V(start): 0.000 \n",
      "iter   12 | diff: 0.04330 | V(start): 0.000 \n",
      "iter   13 | diff: 0.03802 | V(start): 0.033 \n",
      "iter   14 | diff: 0.03332 | V(start): 0.058 \n",
      "iter   15 | diff: 0.02910 | V(start): 0.087 \n",
      "iter   16 | diff: 0.01855 | V(start): 0.106 \n",
      "iter   17 | diff: 0.01403 | V(start): 0.120 \n",
      "iter   18 | diff: 0.00810 | V(start): 0.128 \n",
      "iter   19 | diff: 0.00555 | V(start): 0.133 \n",
      "iter   20 | diff: 0.00321 | V(start): 0.137 \n",
      "iter   21 | diff: 0.00247 | V(start): 0.138 \n",
      "iter   22 | diff: 0.00147 | V(start): 0.139 \n",
      "iter   23 | diff: 0.00104 | V(start): 0.140 \n",
      "iter   24 | diff: 0.00058 | V(start): 0.140 \n",
      "iter   25 | diff: 0.00036 | V(start): 0.141 \n",
      "iter   26 | diff: 0.00024 | V(start): 0.141 \n",
      "iter   27 | diff: 0.00018 | V(start): 0.141 \n",
      "iter   28 | diff: 0.00012 | V(start): 0.141 \n",
      "iter   29 | diff: 0.00007 | V(start): 0.141 \n",
      "iter   30 | diff: 0.00004 | V(start): 0.141 \n",
      "iter   31 | diff: 0.00003 | V(start): 0.141 \n",
      "iter   32 | diff: 0.00001 | V(start): 0.141 \n",
      "iter   33 | diff: 0.00001 | V(start): 0.141 \n",
      "Принято! Алгоритм сходится!\n",
      "Cреднее вознаграждение: 0.741\n",
      "Принято!\n"
     ]
    }
   ],
   "source": [
    "# Получаем среднее вознаграждение агента\n",
    "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        # выполняем оптимальное действие в окружении\n",
    "        # s, r, done, _ = \n",
    "        #~~~~~~~~~~ Решение ~~~~~~~~~~~~~~~\n",
    "        \n",
    "         \n",
    "        s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, 0.9))\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "        rewards.append(r)\n",
    "        if done: break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "    \n",
    "print(\"Cреднее вознаграждение:\", np.mean(total_rewards))\n",
    "assert(0.6 <= np.mean(total_rewards) <= 0.8)\n",
    "print(\"Принято!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xdMVQ1XQsN5s"
   },
   "source": [
    "### Задание 3\n",
    "\n",
    "Теперь рассмотрим следующий алгоритм - итерации по стратегиям (PI):\n",
    "\\begin{enumerate}\n",
    "    \\item Инициализация $\\pi_0$   (например случайные действия)\n",
    "    \\item For $n=0, 1, 2, \\dots$\n",
    "    \\item Вычисление state-value функции $V^{\\pi_{n}}$\n",
    "    \\item Используя $V^{\\pi_{n}}$, рассчитываем функцию $Q^{\\pi_{n}}$\n",
    "    \\item Рассчитываем новую стратегию $\\pi_{n+1}(s) = \\operatorname*{argmax}_a Q^{\\pi_{n}}(s,a)$\n",
    "\\end{enumerate}\n",
    "\n",
    "PI включает в себя оценку полезности состояния как внутренний шаг."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Pg53dh5sN5s"
   },
   "source": [
    "Вначале оценим полезности, используя текущую стратегию:\n",
    "$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n",
    "    Мы будем искать точное решение, хотя могли использовать и предыдущий итерационный подход. Для этого будем решать систему линейных уравнений относительно $V^{\\pi}(s_i)$ с помощью np.linalg.solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sP2gL08osN5u"
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import solve\n",
    "\n",
    "def compute_vpi(mdp, policy, gamma):\n",
    "    \"\"\"\n",
    "    Считем V^pi(s) для всех состояний, согласно стратегии.\n",
    "    :param policy: словарь состояние->действие {s : a}\n",
    "    :returns: словарь {state : V^pi(state)}\n",
    "    \"\"\"\n",
    "    states = mdp.get_all_states()\n",
    "    A, b = [], []\n",
    "    for i, state in enumerate(states):\n",
    "        if state in policy:\n",
    "            a = policy[state]\n",
    "            # формируем матрицу A (... A.append(...))\n",
    "            #~~~~~~~~~~ Решение ~~~~~~~~~~~~~~~\n",
    "            \n",
    "             \n",
    "            A.append([(1-gamma * mdp.get_transition_prob(state, a, s) if i == j else -gamma * mdp.get_transition_prob(state, a, s)) \n",
    "                  for j, s in enumerate(states)])\n",
    "            \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            \n",
    "            # и вектор b (b.append(...))\n",
    "            #~~~~~~~~~~ Решение ~~~~~~~~~~~~~~~\n",
    "            \n",
    "             \n",
    "            b.append(sum([prob * mdp.get_reward(state, a, next_state) \n",
    "                      for next_state, prob in mdp.get_next_states(state, a).items()]))\n",
    "            \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            \n",
    "        else:\n",
    "            # формируем матрицу A (... A.append(...))\n",
    "            #~~~~~~~~~~ Решение ~~~~~~~~~~~~~~~\n",
    "            \n",
    "             \n",
    "            A.append([(1 if i == j else 0) for j, s in enumerate(states)])\n",
    "            \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            \n",
    "            # вектор b (b.append(...))\n",
    "            #~~~~~~~~~~ Решение ~~~~~~~~~~~~~~~\n",
    "            \n",
    "             \n",
    "            b.append(0)\n",
    "            \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            \n",
    "    A = np.array(A)\n",
    "    b = np.array(b)\n",
    "    \n",
    "    values = solve(A, b)\n",
    "    \n",
    "    state_values = {states[i] : values[i] \n",
    "                    for i in range(len(states))}\n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1aG9dcJVsN5w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s0': -0.7320493642483172, 's1': -0.27767389678384435, 's2': -0.8947270007479434}\n"
     ]
    }
   ],
   "source": [
    "transition_probs = {\n",
    "    's0': {\n",
    "        'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}\n",
    "    },\n",
    "    's1': {\n",
    "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}\n",
    "    },\n",
    "    's2': {\n",
    "        'a0': {'s0': 0.4, 's1': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "    }\n",
    "}\n",
    "rewards = {\n",
    "    's1': {'a0': {'s0': +5}},\n",
    "    's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')\n",
    "\n",
    "gamma = 0.9  # коэффициент дисконтирования для MDP\n",
    "\n",
    "test_policy = {\n",
    "    s: np.random.choice(mdp.get_possible_actions(s))\n",
    "    for s in mdp.get_all_states()}\n",
    "new_vpi = compute_vpi(mdp, test_policy, gamma)\n",
    "\n",
    "print(new_vpi)\n",
    "assert type(new_vpi) is dict, \\\n",
    "    \"функция compute_vpi должна возвращать словарь \\\n",
    "    {состояние s : V^pi(s) }\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hD-bBuwKsN5y"
   },
   "source": [
    "Теперь обновляем стратегию на основе новых значений полезностей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5pKbE1visN50"
   },
   "outputs": [],
   "source": [
    "def compute_new_policy(mdp, vpi, gamma):\n",
    "    \"\"\"\n",
    "    Рассчитываем новую стратегию\n",
    "    :param vpi: словарь {state : V^pi(state) }\n",
    "    :returns: словарь {state : оптимальное действие}\n",
    "    \"\"\"\n",
    "    Q = {}\n",
    "    for state in mdp.get_all_states():\n",
    "        Q[state] = {}\n",
    "        for a in mdp.get_possible_actions(state):\n",
    "            values = []\n",
    "            for next_state in mdp.get_next_states(state,\n",
    "                                                  a):\n",
    "                r = mdp.get_reward(state, a, next_state)\n",
    "                p = mdp.get_transition_prob(state, a,\n",
    "                                            next_state)\n",
    "                # values.append(...)\n",
    "                #~~~~~~~~~~ Решение ~~~~~~~~~~~~~~~\n",
    "                \n",
    "                 \n",
    "                values.append(\n",
    "                    p * (r + gamma * vpi[next_state]))\n",
    "                \n",
    "                #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                \n",
    "            Q[state][a] = sum(values)\n",
    "\n",
    "    policy = {}\n",
    "    for state in mdp.get_all_states():\n",
    "        actions = mdp.get_possible_actions(state)\n",
    "        if actions:\n",
    "            # выбираем оптимальное действие в state\n",
    "            # policy[state] = ... \n",
    "            #~~~~~~~~~~ Решение ~~~~~~~~~~~~~~~\n",
    "            \n",
    "             \n",
    "            i = actions[0]\n",
    "            for a in actions:\n",
    "                if Q[state][a] > Q[state][i]:\n",
    "                    i = a\n",
    "            policy[state] = i\n",
    "            \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SKkZnfGRsN53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s0': 'a0', 's1': 'a0', 's2': 'a0'}\n"
     ]
    }
   ],
   "source": [
    "new_policy = compute_new_policy(mdp, new_vpi, gamma)\n",
    "\n",
    "print(new_policy)\n",
    "\n",
    "assert type(new_policy) is dict, \\\n",
    "\"функция compute_new_policy должна возвращать словарь \\\n",
    "{состояние s: оптимальное действие}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHax1qV5sN55"
   },
   "source": [
    "Собираем все в единый цикл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4rfeoFasN56"
   },
   "outputs": [],
   "source": [
    "def policy_iteration(mdp, policy=None, gamma = 0.9, \n",
    "                 num_iter = 1000, min_difference = 1e-5):\n",
    "    \"\"\" \n",
    "    Запускаем цикл итерации по стратегиям \n",
    "    Если стратегия не определена, задаем случайную\n",
    "    \"\"\"\n",
    "    for i in range(num_iter):\n",
    "        if not policy:\n",
    "            policy = {}\n",
    "            for s in mdp.get_all_states():\n",
    "                if mdp.get_possible_actions(s):\n",
    "                    policy[s] = np.random \\\n",
    "                    .choice(mdp.get_possible_actions(s))\n",
    "        # state_values = \n",
    "        #~~~~~~~~~~ Решение ~~~~~~~~~~~~~~~\n",
    "        \n",
    "         \n",
    "        state_values = compute_vpi(mdp, policy, gamma)\n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "        # policy = \n",
    "        #~~~~~~~~~~ Решение ~~~~~~~~~~~~~~~\n",
    "        \n",
    "         \n",
    "        policy = compute_new_policy(mdp, state_values, gamma)    \n",
    "        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "    return state_values, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NJm34Gc5sN57"
   },
   "source": [
    "Тестируем на FrozenLake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NC7hXWhssN58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward:  0.892\n",
      "Принято!\n"
     ]
    }
   ],
   "source": [
    "mdp = FrozenLakeEnv(slip_chance=0.1)\n",
    "state_values, policy = policy_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(policy[s])\n",
    "        rewards.append(r)\n",
    "        if done: break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "    \n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.8 <= np.mean(total_rewards) <= 0.95)\n",
    "print(\"Принято!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "sem2_dp.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
