{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## Deep Q-Network\n", "\n", "\u0421\u0445\u0435\u043c\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 DQN \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0430 \u043d\u0430 \u0440\u0438\u0441\u0443\u043d\u043a\u0435: \n", "<img src=\"files/dqn_rp_diagram.jpg\">\n", "\n", "\u041a\u043b\u044e\u0447\u0435\u0432\u044b\u043c\u0438 \u043e\u0442\u043b\u0438\u0447\u0438\u044f\u043c\u0438 \u043e\u0442 \u043e\u0431\u044b\u0447\u043d\u043e\u0439 \u0430\u043f\u043f\u0440\u043e\u043a\u0441\u0438\u043c\u0430\u0446\u0438\u0438 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u044c\u044e \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0434\u0432\u0430 \u043d\u043e\u0432\u044b\u0445 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u0430: replay buffer \u0438 target network.\n", "\n", "#### 1. Replay buffer\n", "\u0425\u0440\u0430\u043d\u0438\u0442 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e, \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043f\u043e\u043b\u0443\u0447\u0430\u043b \u0430\u0433\u0435\u043d\u0442 \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u0441\u043e \u0441\u0440\u0435\u0434\u043e\u0439. \u041f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e \u0441\u044d\u043c\u043f\u043b\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u044d\u0442\u0443 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u0432 \u0432\u0438\u0434\u0435 \u043a\u043e\u0440\u0442\u0435\u0436\u0435\u0439: $<S_{t-1}, a_{t-1}, r_{t-1}, s_{t}, {done}>$.\n", "\n", "#### 2. Target Network $Q_{\\theta}^{target}$\n", "\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f loss \u0444\u0443\u043d\u043a\u0446\u0438\u0438 Policy Net. \n", "$$L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{\\theta}^{target}(s', a')]) ^2.$$\n", "\u041a\u0430\u0436\u0434\u044b\u0435 $N$ \u0448\u0430\u0433\u043e\u0432 \u043e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0432\u0435\u0441\u0430\u043c\u0438 $Q_{\\theta}$.\n", "\n", "#### 3. Policy Net $Q_{\\theta}$\n", "\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u0441 \u043e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u0435\u043c. \u041e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043d\u0430\u043a\u043e\u043f\u043b\u0435\u043d\u043d\u044b\u0439 \u0432 Replay Buffer \u043e\u043f\u044b\u0442.\n"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["# \u0438\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438\n", "\n", "import gym\n", "import collections\n", "import random\n", "\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torch.optim as optim\n", "\n", "# \u0435\u0441\u043b\u0438 \u0432\u0438\u0434\u0435\u043e\u043a\u0430\u0440\u0442\u0430 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u0430, \u0442\u043e \u0431\u0443\u0434\u0435\u043c \u0435\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "print(\"device:\", device)"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["# \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u043a\u043b\u0430\u0441\u0441 Q-Network\n", "\n", "class QNetwork(nn.Module):\n", "    def __init__(self):\n", "        \"\"\"\n", "        \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0441\u0435\u0442\u0438\n", "        \"\"\"\n", "        super().__init__()\n", "        \n", "        self.fc1 = nn.Linear(4, 256)\n", "        self.fc2 = nn.Linear(256, 2)\n", "\n", "    def forward(self, x):\n", "        \"\"\"\n", "        \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0433\u0440\u0430\u0444\u0430 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439\n", "        :param x: \u0432\u0445\u043e\u0434\n", "        :return: \n", "        \"\"\"\n", "        x = F.relu(self.fc1(x))\n", "        x = self.fc2(x)\n", "        return x\n", "\n", "    def sample_action(self, obs, epsilon):\n", "        \"\"\"\n", "        \u0441\u044d\u043c\u043f\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f\n", "        :param obs: \n", "        :param epsilon: \n", "        :return: \n", "        \"\"\"\n", "        out = self.forward(obs)\n", "        coin = random.random()\n", "        if coin < epsilon:\n", "            return random.randint(0, 1)\n", "        else:\n", "            return out.argmax().item()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \u0417\u0430\u0434\u0430\u043d\u0438\u0435 1. \u0420\u0435\u0430\u043b\u0438\u0437\u0443\u0439\u0442\u0435\u0439 \u043c\u0435\u0442\u043e\u0434\u044b \u043a\u043b\u0430\u0441\u0441\u0430 ReplayBuffer"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["class ReplayBuffer():\n", "\n", "    def __init__(self, max_size):\n", "        \"\"\"\n", "        \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0443 \u0434\u043b\u044f \u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445\n", "        \"\"\"\n", "        #~~~~~~~~~~ \u0420\u0435\u0448\u0435\u043d\u0438\u0435 ~~~~~~~~~~~~~~~\n", "        \n", "         \n", "        self.buffer = collections.deque(maxlen=max_size)\n", "        \n", "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n", "        \n", "\n", "    def put(self, transition):\n", "        \"\"\"\n", "        \u043f\u043e\u043c\u0435\u0449\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 replay buffer\n", "        :param transition: (s, a, r, next_s, done_mask)\n", "        :return:\n", "        \"\"\"\n", "        #~~~~~~~~~~ \u0420\u0435\u0448\u0435\u043d\u0438\u0435 ~~~~~~~~~~~~~~~\n", "        \n", "         \n", "        self.buffer.append(transition)\n", "        \n", "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n", "        \n", "\n", "    def sample(self, n):\n", "        \"\"\"\n", "        \u0441\u044d\u043c\u043f\u043b\u0438\u0440\u0443\u0435\u043c \u0431\u0430\u0442\u0447 \u0437\u0430\u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0430\n", "        :param n: \u0440\u0430\u0437\u043c\u0435\u0440 \u043c\u0438\u043d\u0438-\u0431\u0430\u0442\u0447\u0430\n", "        :return:\n", "        \"\"\"\n", "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n", "\n", "        # \u0441\u044d\u043c\u043f\u043b\u0438\u0440\u0443\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u0431\u0430\u0442\u0447 \u0438 \u0437\u0430\u043f\u043e\u043b\u043d\u044f\u0435\u043c s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst\n", "        #~~~~~~~~~~ \u0420\u0435\u0448\u0435\u043d\u0438\u0435 ~~~~~~~~~~~~~~~\n", "        \n", "         \n", "        mini_batch = random.sample(self.buffer, n)\n", "\n", "        for transition in mini_batch:\n", "            s, a, r, s_prime, done_mask = transition\n", "            s_lst.append(s)\n", "            a_lst.append([a])\n", "            r_lst.append([r])\n", "            s_prime_lst.append(s_prime)\n", "            done_mask_lst.append([done_mask])\n", "        \n", "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n", "        \n", "\n", "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n", "               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n", "               torch.tensor(done_mask_lst)\n", "\n", "    def __len__(self):\n", "        \"\"\"\n", "        \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0440\u0430\u0437\u043c\u0435\u0440 replay buffer'\u0430\n", "        :return: len(replay_buffer)\n", "        \"\"\"\n", "        #~~~~~~~~~~ \u0420\u0435\u0448\u0435\u043d\u0438\u0435 ~~~~~~~~~~~~~~~\n", "        \n", "         \n", "        return len(self.buffer)\n", "        \n", "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n", "        \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \u0417\u0430\u0434\u0430\u043d\u0438\u0435 2. \u0417\u0430\u043f\u043e\u043b\u043d\u0438\u0442\u0435 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438 \u043c\u0435\u0442\u043e\u0434\u0430 train"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["def train(q, q_target, replay_buffer, optimizer, batch_size, gamma, updates_number=10):\n", "    \"\"\"\n", "    \u0442\u0440\u0435\u043d\u0438\u0440\u0443\u0435\u043c \u043d\u0430\u0448\u0443 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0443\n", "    :param q: policy \u0441\u0435\u0442\u044c\n", "    :param q_target: target \u0441\u0435\u0442\u044c\n", "    :param replay_buffer:\n", "    :param optimizer:\n", "    :param batch_size: \u0440\u0430\u0437\u043c\u0435\u0440 \u043c\u0438\u043d\u0438-\u0431\u0430\u0442\u0447\u0430\n", "    :param gamma: \u0434\u0438\u0441\u043a\u043e\u043d\u0442\u0438\u0440\u0443\u044e\u0449\u0438\u0439 \u043c\u043d\u043e\u0436\u0438\u0442\u0435\u043b\u044c\n", "    :param updates_number: \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0432\u044b\u043f\u043e\u043b\u043d\u0438\u0442\u044c\n", "    :return:\n", "    \"\"\"\n", "    for i in range(updates_number):\n", "        # \u0441\u044d\u043c\u043f\u043b\u0438\u0440\u0443\u0435\u043c \u043c\u0438\u043d\u0438-\u0431\u0430\u0442\u0447 \u0438\u0437 replay buffer'\u0430\n", "        s, a, r, s_prime, done_mask = replay_buffer.sample(batch_size)\n", "\n", "        # \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043f\u043e\u043b\u0435\u0437\u043d\u043e\u0441\u0442\u044c, \u0434\u043b\u044f \u0432\u044b\u0431\u0440\u0430\u043d\u043d\u043e\u0433\u043e \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f q \u0441\u0435\u0442\u0438\n", "        # q_a = \n", "        #~~~~~~~~~~ \u0420\u0435\u0448\u0435\u043d\u0438\u0435 ~~~~~~~~~~~~~~~\n", "        \n", "         \n", "        q_out = q(s)\n", "        q_a = q_out.gather(1, a)\n", "        \n", "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n", "        \n", "\n", "        # \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 max_q target \u0441\u0435\u0442\u0438 \u0438 \u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 target\n", "        # target = \n", "        #~~~~~~~~~~ \u0420\u0435\u0448\u0435\u043d\u0438\u0435 ~~~~~~~~~~~~~~~\n", "        \n", "         \n", "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n", "        target = r + gamma * max_q_prime * done_mask\n", "        \n", "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n", "        \n", "\n", "        # \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c loss \u0444\u0443\u043d\u043a\u0446\u0438\u044e, \u0434\u043b\u044f q!\n", "        loss = F.smooth_l1_loss(q_a, target)\n", "\n", "        optimizer.zero_grad()\n", "        loss.backward()\n", "        optimizer.step()"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["def run(learning_rate, gamma, buffer_max_size, batch_size, target_update_interval,\n", "        replay_buffer_start_size, print_interval=20, n_episodes=2000):\n", "    # \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u043e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u0435\n", "    env = gym.make('CartPole-v1')\n", "\n", "    # \u0441\u043e\u0437\u0434\u0430\u0435\u043c q \u0438 target_q\n", "    q = QNetwork()\n", "    q_target = QNetwork()\n", "\n", "    # \u043a\u043e\u043f\u0438\u0440\u0443\u0435\u043c \u0432\u0435\u0441\u0430 q \u0432 target_q\n", "    q_target.load_state_dict(q.state_dict())\n", "\n", "    # \u0441\u043e\u0437\u0434\u0430\u0435\u043c replay buffer\n", "    replay_buffer = ReplayBuffer(max_size=buffer_max_size)\n", "\n", "    score = 0.0\n", "\n", "    # \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u043c lr\n", "    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n", "\n", "    for n_epi in range(n_episodes):\n", "\n", "        # \u043f\u043e\u0441\u0442\u0435\u043f\u0435\u043d\u043d\u043e \u0438\u0437\u043c\u0435\u043d\u044f\u0435\u043c eps \u0441 8% \u0434\u043e 1%\n", "        epsilon = max(0.01, 0.08 - 0.01 * (n_epi / 200))\n", "\n", "        s = env.reset()\n", "\n", "        # \u0432\u044b\u043f\u043e\u043b\u044f\u043d\u0435\u043c 600 \u0448\u0430\u0433\u043e\u0432 \u0432 \u043e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u0438 \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\n", "        for t in range(600):\n", "\n", "            # \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0435, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0441\u0435\u0442\u044c q\n", "            a = q.sample_action(torch.from_numpy(s).float(), epsilon)\n", "\n", "            # \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0435 \u0432 \u043e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u0438\n", "            s_prime, r, done, info = env.step(a)\n", "\n", "            # \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 replay buffer\n", "            done_mask = 0.0 if done else 1.0\n", "            \n", "            # \u0441\u0436\u0438\u043c\u0430\u0435\u043c \u0432\u043e\u0437\u043d\u0430\u0433\u0440\u0430\u0436\u0434\u0435\u043d\u0438\u044f \u0438 \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0432 replay buffer\n", "            replay_buffer.put((s, a, r / 100.0, s_prime, done_mask))\n", "\n", "            s = s_prime\n", "\n", "            score += r\n", "\n", "            if done:\n", "                break\n", "\n", "        if len(replay_buffer) > replay_buffer_start_size:\n", "            train(q, q_target, replay_buffer, optimizer, batch_size, gamma)\n", "\n", "        if n_epi % target_update_interval == 0 and n_epi != 0:\n", "            q_target.load_state_dict(q.state_dict())\n", "\n", "        if n_epi % print_interval == 0 and n_epi != 0:\n", "            print(\"# of episode :{}, avg score : {:.1f}, buffer size : {}, epsilon : {:.1f}%\".format(\n", "                n_epi, score / print_interval, len(replay_buffer), epsilon * 100))\n", "            score = 0.0\n", "    env.close()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \u0417\u0430\u0434\u0430\u043d\u0438\u0435 3. \u041f\u0440\u043e\u0432\u0435\u0434\u0438\u0442\u0435 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u044b \u0441 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u043c\u0438 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438, \u0434\u043b\u044f \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u044b"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["# \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b\n", "run(learning_rate=0.0005,\n", "    gamma=0.98,\n", "    buffer_max_size=50000,\n", "    batch_size=32,\n", "    target_update_interval=10,\n", "    replay_buffer_start_size=2000)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### \u0417\u0430\u0434\u0430\u043d\u0438\u0435 4. \u0423\u0431\u0435\u0440\u0438\u0442\u0435 \u0438\u0437 \u043c\u043e\u0434\u0435\u043b\u0438 Target Net \u0438 \u0441\u0440\u0430\u0432\u043d\u0438\u0442\u0435, \u043a\u0430\u043a \u044d\u0442\u043e \u0432\u043b\u0438\u044f\u0435\u0442 \u043d\u0430 \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f (\u043f\u043e\u0441\u0442\u0440\u043e\u0439\u0442\u0435 \u0433\u0440\u0430\u0444\u0438\u043a \u0441\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## \u0414\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u043c\u0430\u0442\u0435\u0440\u0438\u0430\u043b\u044b:\n", "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.3"}}, "nbformat": 4, "nbformat_minor": 2}