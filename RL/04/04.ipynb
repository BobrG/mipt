{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AQpuhSkBTKjE"
   },
   "source": [
    "## Аппроксимация Q-функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3KgKLUiJTKjI"
   },
   "source": [
    "В этой тетрадке мы будем использовать библиотеку tensorflow для обучения нейронной сети, хотя можно использовать и любую другую библиотеку. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SDsUrk4tTKjK"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7IMFc1dTTKjP"
   },
   "source": [
    "Будем тестировать наши модели на классической задаче с перевернутым маятником:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8pb2lfxTKjR",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1b4fbcda20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vX54sySbTKjV"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bk3-9NN5TKjZ"
   },
   "source": [
    "# Глубокое Q-обучение: построение сети\n",
    "\n",
    "Так как описание состояния в задаче с маятником представляет собой не \"сырые\" признаки, а уже предобработанные (координаты, углы), нам не нужна для начала сложная архитектура, начнем с такой:\n",
    "\n",
    "<img src=\"qlearningscheme.png\" caption=\"Архитектура сети\">\n",
    "Для начала попробуйте использовать только полносвязные слои (__L.Dense__) и линейные активационные функции. Сигмоиды и другие функции не будут работать с ненормализованными входными данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KHi0meFMTKja",
    "outputId": "3d9db427-b07f-4b40-a9ee-9b643410def4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers as L\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M6ZJDOPJTKjd"
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2568adc96a96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# строим сеть!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#~~~~~~~~ Ваш код здесь ~~~~~~~~~~~\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "network = keras.models.Sequential()\n",
    "network.add(L.InputLayer(state_dim))\n",
    "# строим сеть!\n",
    "#~~~~~~~~ Ваш код здесь ~~~~~~~~~~~\n",
    "raise NotImplementedError\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kkOAJbTHTKji"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def get_action(state, epsilon=0):\n",
    "    \"\"\"\n",
    "    сэмплируем (eps greedy) действие  \n",
    "    \"\"\"\n",
    "    q_values = network.predict(state[None])[0]\n",
    "    \n",
    "    ### Ваш код здесь - нужно выбрать действия eps-жадно\n",
    "    # action = \n",
    "    #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~    \n",
    "    raise NotImplementedError    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "zsTrcYTSTKjk",
    "outputId": "86e4215c-366f-48fd-ee6f-e094053aa4b8"
   },
   "outputs": [],
   "source": [
    "assert network.output_shape == (None, n_actions), \\\n",
    "    \"убедитесь, что стратегия переводит \\\n",
    "    s -> [Q(s,a0), ..., Q(s, a_last)]\"\n",
    "assert network.layers[-1].activation == \\\n",
    "       keras.activations.linear, \\\n",
    "    \"убедитесь, что вы предсказываете q без нелинейности\"\n",
    "# проверяем исследование\n",
    "s = env.reset()\n",
    "assert np.shape(get_action(s)) == (), \\\n",
    "    \"убедитесь, что возвращаете одно действие\"\n",
    "for eps in [0., 0.1, 0.5, 1.0]:\n",
    "    na = n_actions\n",
    "    st = np.bincount([get_action(s, epsilon=eps) \\\n",
    "                      for i in range(10000)],\n",
    "                     minlength=na)\n",
    "    ba = st.argmax()\n",
    "    assert abs(\n",
    "        st[ba] - 10000 * (1 - eps + eps / na)) < 200\n",
    "    for oa in range(na):\n",
    "        if oa != ba:\n",
    "            assert abs(st[oa] - 10000 * (eps / na)) < 200\n",
    "    print('e=%.1f tests passed' % eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YyKpjfO7TKjq"
   },
   "source": [
    "### Q-обучение через градиентный спуск\n",
    "\n",
    "Теперь будем приближать Q-функцию агента, минимизируя TD функцию потерь:\n",
    "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2. $$\n",
    "\n",
    "Основная тонкость состоит в использовании  $Q_{-}(s',a')$. Эта та же самая функция, что и $Q_{\\theta}$, которая является выходом нейронной сети, но при обучении сети, мы не пропускаем через эти слои градиенты. Для этого используется функция `tf.stop_gradient`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nnd2924Md24U"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOw0pH5WTKjs"
   },
   "outputs": [],
   "source": [
    "# Создаем placeholders для <s, a, r, s'>, \n",
    "# а также индикатор окончания эпизода (is_done = True)\n",
    "states_ph = tf.placeholder('float32', \n",
    "                           shape=(None,) + state_dim)\n",
    "actions_ph = tf.placeholder('int32', shape=[None])\n",
    "rewards_ph = tf.placeholder('float32', shape=[None])\n",
    "next_states_ph = tf.placeholder('float32', \n",
    "                           shape=(None,) + state_dim)\n",
    "is_done_ph = tf.placeholder('bool', shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L6MRQ2z6TKjx"
   },
   "outputs": [],
   "source": [
    "# получаем q для всех действий, в текущем состоянии\n",
    "predicted_qvalues = network(states_ph)\n",
    "\n",
    "# получаем q-values для выбранного действия\n",
    "predicted_qvalues_for_actions =\\\n",
    "tf.reduce_sum(\n",
    "predicted_qvalues * tf.one_hot(actions_ph, n_actions),\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T0Sb5q9uTKjz"
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "# применяем сеть для получения q-value для next_states_ph\n",
    "# predicted_next_qvalues =\n",
    "#~~~~~~~~ Ваш код здесь ~~~~~~~~~~~\n",
    "raise NotImplementedError\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# вычисляем V*(next_states) \n",
    "# по предсказанным следующим q-values\n",
    "# next_state_values =\n",
    "#~~~~~~~~ Ваш код здесь ~~~~~~~~~~~\n",
    "raise NotImplementedError\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# Вычисляем target q-values для функции потерь \n",
    "# target_qvalues_for_actions = \n",
    "#~~~~~~~~ Ваш код здесь ~~~~~~~~~~~\n",
    "raise NotImplementedError\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# для последнего действия используем \n",
    "# упрощенную формулу Q(s,a) = r(s,a) \n",
    "target_qvalues_for_actions =\\\n",
    "tf.where(is_done_ph, rewards_ph, \n",
    "         target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aA15Fi_8d4hL"
   },
   "source": [
    "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2. $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LK9euKgoTKj2"
   },
   "outputs": [],
   "source": [
    "### среднеквадратичная функция потерь stop_gradient\n",
    "# loss = \n",
    "#~~~~~~~~ Ваш код здесь ~~~~~~~~~~~\n",
    "raise NotImplementedError\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# применяем AdamOptimizer\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5oZQ957tTKj5"
   },
   "outputs": [],
   "source": [
    "assert tf.gradients(loss, \\\n",
    "    [predicted_qvalues_for_actions])[0] is not None, \\\n",
    "\"убедитесь, что обновление выполняется\\\n",
    "только для выбранного действия\"\n",
    "assert tf.gradients(loss, \\\n",
    "    [predicted_next_qvalues])[0] is None, \\\n",
    "\"убедитесь, что вы не распространяете градиент Q_(s',a')\"\n",
    "assert predicted_next_qvalues.shape.ndims == 2, \\\n",
    "\"убедитесь, что вы предсказываете q для всех действий,\\\n",
    "следующего состояния\"\n",
    "assert next_state_values.shape.ndims == 1, \\\n",
    "\"убедитесь, что вы вычислили V(s') как максимум\\\n",
    "только по оси действий, а не по всем осям\"\n",
    "assert target_qvalues_for_actions.shape.ndims == 1, \\\n",
    "\"что-то не так с целевыми q-значениями,\\\n",
    "они должны быть вектором\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "adITE6BaTKj-"
   },
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YWIDEK7ATKkA"
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000, epsilon=0, train=False):\n",
    "    \"\"\"генерация сессии\"\"\"\n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        a = get_action(s, epsilon=epsilon)       \n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        \n",
    "        if train:\n",
    "            sess.run(train_step,{\n",
    "                states_ph: [s], actions_ph: [a], \n",
    "                rewards_ph: [r], next_states_ph: [next_s], \n",
    "                is_done_ph: [done]\n",
    "            })\n",
    "\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TCeeMefwTKkC"
   },
   "outputs": [],
   "source": [
    "epsilon = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "-fMjfHxiTKkE",
    "outputId": "cc8bb4c7-37ff-49f8-bb38-ed132adbc730",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    session_rewards = [generate_session(epsilon=epsilon, \n",
    "                        train=True) for _ in range(500)]\n",
    "    print(\"epoch #{}\\tmean r = {:.3f}\\tepsilon = {:.3f}\"\n",
    "          .format(i, np.mean(session_rewards), epsilon))\n",
    "    \n",
    "    epsilon *= 0.95\n",
    "    epsilon = max(0.1, epsilon)\n",
    "    assert epsilon >= 1e-4, \\\n",
    "    \"убедитесь, что epsilon не становится < 0\"\n",
    "    if np.mean(session_rewards) > 300:\n",
    "        print (\"Принято!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FSC00yvzTKkH"
   },
   "source": [
    "### Интерпретация результатов\n",
    "\n",
    "\\begin{itemize}\n",
    "\\item mean reward -- среднее вознаграждение за эпизод. В случае корректной реализации, этот показатель будет низким первые 5 эпох и только затем будет возрастать и сойдется на 20-30 эпох в зависииости от архитектуры сети.\n",
    "\\item Если сеть не достигает нужных результатов к концу цикла, попробуйте увеличить число нейронов в скрытом слое или поменяйте $\\epsilon$.\n",
    "\\item epsilon -- обеспечивает стремление агента исследовать среду. Можно искусственно изменять малые значения $\\epsilon$ при низких результатах на 0.1 - 0.5.\n",
    "\\end{itemize}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sem4_approx.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
