{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## Марковский процесс принятий решений\n",
    "\n",
    "Обучение с подкреплением (RL) является направлением машинного обучения и изучает взаимодействие агента, которому необходимо максимизировать долговременный выигрыш в некоторой среде. Агенту не сообщается сведений о правильности действий, как в большинстве задач машинного обучения, вместо этого агент должен определить выгодные действия самостоятельно испробовав их. Испытание действий и отсроченная награда являются основными отличительными признаками RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "<img src=\"rlIntro.png\" caption=\"Взаимодействия агента со средой\" style=\"width: 300px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Основные составляющие модели RL:\n",
    "\\begin{itemize}\n",
    "    \\item $s_t$ -- состояние среды в момент времени $t$,\n",
    "    \\item $a_t$ -- действие, совершаемое агентом в момент времени $t$,\n",
    "    \\item $r_t$ -- вознаграждение, получаемое агентом при совершении действия $a_t$,\n",
    "    \\item $\\pi$ -- стратегия, отвечает за выбор действия в конкретном состоянии.\n",
    "\\end{itemize}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "В простейших моделях RL среда представляется в виде марковского процесса принятия решений (MDP), где функция перехода определяется как $P(s' |s,a)$, что означает вероятность оказаться в состоянии $s'$ при совершении действия $a$ в состоянии $s$. Вознаграждение теперь определяется как $r(s,a,s')$.\n",
    "\n",
    "<img src=\"mdp.png\" caption=\"Марковский процесс принятия решений\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Будем пользоваться стандартными средами, реализованными в библиотеке OpenAI Gym (https://gym.openai.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gym==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym\n",
    "# создаем окружение\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "# рисуем картинку\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Интерфейс среды в OpenAI gym\n",
    "\n",
    "Основные методы класса Env:\n",
    "\\begin{itemize}\n",
    "    \\item reset() - инициализация окружения, возвращает первое наблюдение\n",
    "    \\item render() - визуализация текущего состояния среды\n",
    "    \\item step($a$) - выполнить в среде действие a и получить: new observation - новое наблюдение после выполнения действия $a$; reward - вознаграждение за выполненное действие $a$; $is\\_done$ - True, если процесс завершился, False иначе; $info$ - дополнительная информация\n",
    "\\end{itemize}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "obs0 = env.reset()\n",
    "print(\"изначальное состояние среды:\", obs0)\n",
    "# выполняем действие 2 \n",
    "new_obs, reward, is_done, _ = env.step(2)\n",
    "print(\"новое состояние:\", new_obs, \n",
    "      \"вознаграждение\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Задание 1\n",
    "Наша цель, чтобы тележка достигла флага. Модифицируйте код ниже для выполнения этого задания:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def act(s):\n",
    "    actions = {'left': 0, 'stop': 1, 'right': 2}\n",
    "    # в зависимости от полученного состояния среды \n",
    "    # выбираем действия так, чтобы тележка достигла флага\n",
    "    # action = actions['left'] \n",
    "    #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~    \n",
    "    raise NotImplementedError    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    return action\n",
    "\n",
    "# создаем окружение, с ограничением на число шагов в 249\n",
    "env = gym.wrappers.TimeLimit(\n",
    "    gym.make(\"MountainCar-v0\").unwrapped,\n",
    "    max_episode_steps=250)\n",
    "# проводим инициализацию и запоминаем начальное состояние\n",
    "s = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    # выполняем действие, получаем s, r, done\n",
    "    s, r, done, _ = env.step(act(s))\n",
    "    # визуализируем окружение\n",
    "    env.render()\n",
    "\n",
    "env.close()\n",
    "if s[0] > 0.47:\n",
    "    print(\"Задание выполнено!\")\n",
    "else:\n",
    "    raise NotImplementedError(\"\"\"\n",
    "    Исправьте функцию выбора действия!\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {}
   },
   "source": [
    "### Вероятностный подход к RL\n",
    "\n",
    "Пусть наша стратегия - это вероятностное распределение:\n",
    "\n",
    "$\\pi(s,a) = P(a|s)$\n",
    "\n",
    "Рассмотрим пример с задачей Taxi [Dietterich, 2000]. Для нее мы можем считать, что наша стратегия - это двумерный массив."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v2\")\n",
    "env.reset()\n",
    "env.render()\n",
    "n_states  = env.observation_space.n\n",
    "n_actions = env.action_space.n  \n",
    "print(\"состояний:\", n_states, \"\\nдействий: \", n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Задание 2\n",
    "\n",
    "Создадим \"равномерную\" стратегию в виде двумерного массива с равномерным распределением по действиям и сгенерируем игровую сессию с такой стратегией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "policy = np.array(\n",
    "    [[1./n_actions for _ in range(n_actions)] \n",
    "     for _ in range(n_states)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def generate_session(policy,t_max=10**4):\n",
    "    states,actions = [],[]\n",
    "    total_reward = 0.\n",
    "    s = env.reset()\n",
    "    for t in range(t_max):\n",
    "        # Нужно выбрать действие с вероятностью, \n",
    "        # указанной в стратегии\n",
    "        # a = \n",
    "        #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~        \n",
    "        raise NotImplementedError        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        # запоминаем состояния, действия и вознаграждение\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "        \n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states,actions,total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "s,a,r = generate_session(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Наша задача - выделить лучшие действия и состояния, т.е. такие, при которых было лучшее вознаграждение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def select_elites(states_batch, actions_batch, \n",
    "                  rewards_batch, percentile=50):\n",
    "    \"\"\"\n",
    "    Выбирает состояния и действия с заданным процентилем \n",
    "    :param states_batch: states_batch[sess_i][t]\n",
    "    :param actions_batch: actions_batch[sess_i][t]\n",
    "    :param rewards_batch: rewards_batch[sess_i]\n",
    "    \n",
    "    :returns: elite_states, elite_actions - одномерные \n",
    "    списки состояния и действия, выбранных сессий\n",
    "    \"\"\"\n",
    "    # нужно найти порог вознаграждения по процентилю\n",
    "    # reward_threshold =\n",
    "    #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~    \n",
    "    raise NotImplementedError    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    \n",
    "    # в соответствии с найденным порогом - отобрать \n",
    "    # подходящие состояния и действия\n",
    "    # elite_states = \n",
    "    # elite_actions = \n",
    "    #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~    \n",
    "    raise NotImplementedError    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    \n",
    "    return elite_states,elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "states_batch = [\n",
    "    [1, 2, 3],  # game1\n",
    "    [4, 2, 0, 2],  # game2\n",
    "    [3, 1]  # game3\n",
    "]\n",
    "\n",
    "actions_batch = [\n",
    "    [0, 2, 4],  # игра 1\n",
    "    [3, 2, 0, 1],  # игра 2\n",
    "    [3, 3]  # игра 3\n",
    "]\n",
    "rewards_batch = [\n",
    "    3,  # игра 1\n",
    "    4,  # игра 2\n",
    "    5,  # игра 3\n",
    "]\n",
    "\n",
    "test_result_0 = select_elites(states_batch, actions_batch,\n",
    "                              rewards_batch, percentile=0)\n",
    "test_result_40 = select_elites(states_batch,\n",
    "                               actions_batch,\n",
    "                               rewards_batch,\n",
    "                               percentile=30)\n",
    "test_result_90 = select_elites(states_batch,\n",
    "                               actions_batch,\n",
    "                               rewards_batch,\n",
    "                               percentile=90)\n",
    "test_result_100 = select_elites(states_batch,\n",
    "                                actions_batch,\n",
    "                                rewards_batch,\n",
    "                                percentile=100)\n",
    "\n",
    "assert np.all(\n",
    "    test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1]) \\\n",
    "       and np.all(\n",
    "    test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]), \\\n",
    "    \"Для процентиля 0 необходимо выбрать все состояния \" \\\n",
    "    \"и действия в хронологическом порядке\"\n",
    "\n",
    "assert np.all(test_result_40[0] == [4, 2, 0, 2, 3, 1])\\\n",
    "   and np.all(test_result_40[1] == [3, 2, 0, 1, 3, 3]), \\\n",
    "    \"Для процентиля 30 необходимо выбрать \" \\\n",
    "    \"состояния/действия из [3:]\"\n",
    "assert np.all(test_result_90[0] == [3, 1]) and \\\n",
    "       np.all(test_result_90[1] == [3, 3]), \\\n",
    "    \"Для процентиля 90 необходимо выбрать состояния \" \\\n",
    "    \"действия одной игры\"\n",
    "assert np.all(test_result_100[0] == [3, 1]) and \\\n",
    "       np.all(test_result_100[1] == [3, 3]), \\\n",
    "    \"Проверьте использование знаков: >=,  >. \" \\\n",
    "    \"Также проверьте расчет процентиля\"\n",
    "print(\"Тесты пройдены!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Теперь мы хотим написать обновляющуюся стратегию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def update_policy(elite_states,elite_actions):\n",
    "    \"\"\"\n",
    "    обновление стратегии\n",
    "    policy[s_i,a_i] ~ #[вхождения  si/ai \n",
    "    в лучшие states/actions]\n",
    "    :param elite_states:  список состояний\n",
    "    :param elite_actions: список действий\n",
    "    \"\"\"\n",
    "    new_policy = np.zeros([n_states,n_actions])\n",
    "    for state in range(n_states):\n",
    "        # обновялем стратегию - нормируем новые частоты \n",
    "        # действий и не забываем про не встречающиеся \n",
    "        # состояния\n",
    "        # new_policy[state, a] = \n",
    "        #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~        \n",
    "        raise NotImplementedError        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "elite_states, elite_actions = (\n",
    "    [1, 2, 3, 4, 2, 0, 2, 3, 1],\n",
    "    [0, 2, 4, 3, 2, 0, 1, 3, 3])\n",
    "\n",
    "new_policy = update_policy(elite_states, elite_actions)\n",
    "\n",
    "assert np.isfinite(\n",
    "    new_policy).all(), \"Стратегия не должна содержать \" \\\n",
    "                       \"NaNs или +-inf. Проверьте \" \\\n",
    "                       \"деление на ноль. \"\n",
    "assert np.all(\n",
    "    new_policy >= 0), \"Стратегия не должна содержать \" \\\n",
    "                      \"отрицательных вероятностей \"\n",
    "assert np.allclose(new_policy.sum(axis=-1),\n",
    "                   1), \"Суммарная\\ вероятность действий\"\\\n",
    "                       \"для состояния должна равняться 1\"\n",
    "reference_answer = np.array([\n",
    "    [1., 0., 0., 0., 0.],\n",
    "    [0.5, 0., 0., 0.5, 0.],\n",
    "    [0., 0.33333333, 0.66666667, 0., 0.],\n",
    "    [0., 0., 0., 0.5, 0.5]])\n",
    "assert np.allclose(new_policy[:4, :5], reference_answer)\n",
    "print(\"Тесты пройдены!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Визуализириуем наш процесс обучения и также будем измерять распределение получаемых за сессию вознаграждений "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def show_progress(rewards_batch, log, reward_range=None):\n",
    "    \"\"\"\n",
    "    Удобная функция, которая отображает прогресс обучения.\n",
    "    Здесь нет крутой математики, только графики.\n",
    "    \"\"\"\n",
    "\n",
    "    if reward_range is None:\n",
    "        reward_range = [-990, +10]\n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    threshold = np.percentile(rewards_batch, percentile)\n",
    "    log.append([mean_reward, threshold])\n",
    "\n",
    "    clear_output(True)\n",
    "    print(\"mean reward = %.3f, threshold=%.3f\" % (\n",
    "        mean_reward,\n",
    "        threshold))\n",
    "    plt.figure(figsize=[8, 4])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
    "    plt.plot(list(zip(*log))[1],\n",
    "             label='Reward thresholds')\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(rewards_batch, range=reward_range)\n",
    "    plt.vlines([np.percentile(rewards_batch, percentile)],\n",
    "               [0], [100], label=\"percentile\",\n",
    "               color='red')\n",
    "    plt.legend(loc=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "policy = np.ones([n_states,n_actions])/n_actions \n",
    "n_sessions = 250  # количество сессий для сэмплирования\n",
    "percentile = 50  # процентиль \n",
    "learning_rate = 0.5  \n",
    "\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "    # генерируем n_sessions сессий\n",
    "    # sessions = []\n",
    "    #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~    \n",
    "    raise NotImplementedError    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    states_batch,actions_batch,rewards_batch = \\\n",
    "        zip(*sessions)\n",
    "    # отбираем лучшие действия и состояния ###\n",
    "    # elite_states, elite_actions = \n",
    "    #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~    \n",
    "    raise NotImplementedError    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    \n",
    "    # обновляем стратегию\n",
    "    # new_policy =\n",
    "    #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~    \n",
    "    raise NotImplementedError    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    policy = learning_rate*new_policy + \\\n",
    "                     (1-learning_rate)*policy\n",
    "    # визуализация обучения\n",
    "    show_progress(rewards_batch,log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Задание 3\n",
    "\n",
    "Попробуем заменить метод обновления вероятностей на нейронную сеть. Будем тестировать нашего нового агента на известной задаче перевернутого маятника с непрерывным множеством действий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\").env  \n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# создаем агента\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# создаем полносвязную сеть с двумя слоями по 20 нейронов, \n",
    "# активация tanh\n",
    "# agent = \n",
    "#~~~~~~~~ Ваш код здесь ~~~~~~~~~~~\n",
    "raise NotImplementedError\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "agent.fit([env.reset()]*n_actions, range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \n",
    "    states,actions = [],[]\n",
    "    total_reward = 0\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        # предсказываем вероятности действий по сети и \n",
    "        # выбираем одно действие\n",
    "        # probs = \n",
    "        # a = \n",
    "        #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~        \n",
    "        raise NotImplementedError        \n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "    return states,actions,total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "n_sessions = 100\n",
    "percentile = 70\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "    # генерируем n_sessions сессий\n",
    "    # sessions = [<gen a list>]\n",
    "    #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~    \n",
    "    raise NotImplementedError    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    states_batch,actions_batch,rewards_batch =\\\n",
    "    map(np.array,zip(*sessions))\n",
    "    \n",
    "    # отбираем лучшие действия и состояния\n",
    "    # elite_states, elite_actions =\n",
    "    #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~    \n",
    "    raise NotImplementedError    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    # обновляем стратегию для предсказания\n",
    "    # elite_actions(y) из elite_states(X)\n",
    "    #~~~~~~~~ Ваш код здесь ~~~~~~~~~~~    \n",
    "    raise NotImplementedError    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    r_range =[0,np.max(rewards_batch)]\n",
    "    show_progress(rewards_batch, log, r_range )\n",
    "    \n",
    "    if np.mean(rewards_batch) > 190:\n",
    "        print(\"Принято!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# монитор для сессий\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n",
    "                           directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# можем посмотреть видео\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),\n",
    "                          os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))\n",
    "# вместо последнего можно выбрать любой индекс"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
