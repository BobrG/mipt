{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Планирование и RL\n",
    "## Алгоритмы Dyna-Q\n",
    "\n",
    "На данном семинаре мы вначале напишем класс DynaQAgent, котрый будет представлять из себя табличного Dyna-Q агента. Проведем его сравнение с Q-learning агентом. Все варианты рассмотрим на примере среды Taxi-v2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала импортируем необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import gym\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dyna-Q\n",
    "\n",
    "Создадим класс DynaQAgent, который будет решать задачу обучения методом Dyna-Q, согласно приведенному ниже алгоритму.\n",
    "\n",
    "<img src=\"dyna.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQAgent:\n",
    "    def __init__(self, alpha, epsilon, discount, n_steps, get_legal_actions):\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "        # создаем структуру данных для хранения информации о модели\n",
    "        # self._memory_model =\n",
    "        ###### Your code here ##########\n",
    "        raise NotImplementedError\n",
    "        ################################\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        return self._qvalues[state][action]\n",
    "\n",
    "    def set_qvalue(self, state, action, value):\n",
    "        self._qvalues[state][action] = value\n",
    "\n",
    "    def get_value(self, state):\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return 0.0\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        value = max(self.get_q_value(state, action) for action in possible_actions)\n",
    "\n",
    "        return value\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        array_qvalues = [self.get_q_value(state, action) for action in possible_actions]\n",
    "        return possible_actions[array_qvalues.index(max(array_qvalues))]\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # Pick Action\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "        action = None\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        # agent parameters:\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        if random.random() > epsilon:\n",
    "            chosen_action = self.get_best_action(state)\n",
    "        else:\n",
    "            chosen_action = random.choice(possible_actions)\n",
    "\n",
    "        return chosen_action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1. Пишем функцию обновления. Здесь же запоминаем необходимую информацию для DYNA-Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, state, action, reward, next_state, use_model=True):\n",
    "    # считаем обновление q\n",
    "    # q =\n",
    "    ###### Your code here ##########\n",
    "    raise NotImplementedError\n",
    "    ################################\n",
    "    self.set_qvalue(state, action, q)\n",
    "\n",
    "    # сохраняем информацию о модели\n",
    "    ###### Your code here ##########\n",
    "    raise NotImplementedError\n",
    "    ################################\n",
    "\n",
    "    # запускаем планирование, если это был внешний вызов функции\n",
    "    if use_model:\n",
    "        self.planning()\n",
    "\n",
    "\n",
    "DynaQAgent.update = update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2. Следующий шаг - этап планирования (поиска). Реализуем функцию planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def planning(self):\n",
    "    # делаем n_steps шагов планирования, пользуемся функцией self.update(..., use_model=False)\n",
    "    ###### Your code here ##########\n",
    "    raise NotImplementedError\n",
    "    ################################\n",
    "\n",
    "\n",
    "DynaQAgent.planning = planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение с Q-learning\n",
    "\n",
    "Сравним получившийся алгоритм с классическим Q-learning на примере Taxi-v2 enviroment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся функцией play_and_train, которая будет полностью прогонять игру, тренировать агента на каждой паре состояние-действие и возвращать полученную награду."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_train(env, agent, t_max=10**4):\n",
    "    \"\"\"This function should \n",
    "    - run a full game, actions given by agent.getAction(s)\n",
    "    - train agent using agent.update(...) whenever possible\n",
    "    - return total reward\"\"\"\n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = agent.get_action(s)\n",
    "\n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        agent.update(s, a, r, next_s)\n",
    "\n",
    "        s = next_s\n",
    "        total_reward += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод run запускает сравнение алгоритмов и русует графики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(alpha=0.1, epsilon=0.3, discount=0.9, planning_steps=10):\n",
    "    env = gym.make(\"Taxi-v2\")\n",
    "    n_actions = env.action_space.n\n",
    "    env.render()\n",
    "\n",
    "    from qlearning import QLearningAgent\n",
    "\n",
    "    agent_ql = QLearningAgent(alpha=alpha, epsilon=epsilon, discount=discount,\n",
    "                              get_legal_actions=lambda s: range(n_actions))\n",
    "\n",
    "    agent_dynaq = DynaQAgent(alpha=alpha, epsilon=epsilon, discount=discount, n_steps=planning_steps,\n",
    "                             get_legal_actions=lambda s: range(n_actions))\n",
    "\n",
    "    rewards_dynaq, rewards_ql = [], []\n",
    "\n",
    "    for i in range(1000):\n",
    "        rewards_dynaq.append(play_and_train(env, agent_dynaq))\n",
    "        rewards_ql.append(play_and_train(env, agent_ql))\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            clear_output(True)\n",
    "            print('DYNA-Q mean reward =', np.mean(rewards_dynaq[-100:]))\n",
    "            print('QLEARNING mean reward =', np.mean(rewards_ql[-100:]))\n",
    "            plt.plot(moving_average(rewards_dynaq), label='dyna-q')\n",
    "            plt.plot(moving_average(rewards_ql), label='q-learning')\n",
    "            plt.grid()\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание  3. Проведите сравнение алгоритмов с различными гиперпараметрами. Добавьте несколько вариантов DYNA-Q с разным числом шагов планирования. Как количество шагов планирования влияет на итоговую сходимость?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(alpha=0.5, epsilon=0.1, discount=0.99, planning_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4. Если вы справились с предыдущими пунктами и во всем разобрались, то приступайте к изучению MCTS, который будет темой нашего следующего семинара!\n",
    "\n",
    "[A Survey of Monte Carlo Tree Search Methods](http://www.incompleteideas.net/609%20dropbox/other%20readings%20and%20resources/MCTS-survey.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
